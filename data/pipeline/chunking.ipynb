{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chunking (and phrase merges)\n",
    "\n",
    "The ETCBC data is not granular enough for many types of searches beneath the phrase level. For example, if there are coordinated noun phrases that all function as a single phrase, the individual phrases are not delineated. I need them spliced out so that I can track coordinated nouns in a phrase. Another case is with quantifiers, wherein the quantifier chains themselves are not in any way set apart from other items in the phrase. For these cases, I will make \"chunk\" objects—these are essentially phrase-like objects.\n",
    "\n",
    "Another problem is that some phrases are split into two, whereas elsewhere in the database the same phrase pattern is portrayed as a single phrase. This is fixed by creating a new object, `phrase2`. Later on, another object will be generated, `cx` (construction), which will contain any mixture of single words, phrases, and sentences, essentially ignoring the old, strict divisions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update Function Features\n",
    "\n",
    "Ensure we're working with up-to-date functions. I am using modified ETCBC functions in this project. Ignore the error message about dependency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(null): can't open file 'remap_phrase_functions.py': [Errno 2] No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!python remap_phrase_functions.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load TF / BHSA Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is Text-Fabric 7.6.8\n",
      "Api reference : https://annotation.github.io/text-fabric/Api/Fabric/\n",
      "\n",
      "119 features found and 1 ignored\n",
      "  0.00s loading features ...\n",
      "   |     0.18s B g_cons_utf8          from /Users/cody/text-fabric-data/etcbc/bhsa/tf/c\n",
      "   |     0.11s B lex                  from /Users/cody/text-fabric-data/etcbc/bhsa/tf/c\n",
      "   |     0.13s B vs                   from /Users/cody/text-fabric-data/etcbc/bhsa/tf/c\n",
      "   |     0.12s B vt                   from /Users/cody/text-fabric-data/etcbc/bhsa/tf/c\n",
      "   |     0.12s B pdp                  from /Users/cody/text-fabric-data/etcbc/bhsa/tf/c\n",
      "   |     0.14s B gloss                from /Users/cody/text-fabric-data/etcbc/bhsa/tf/c\n",
      "   |     0.24s B typ                  from /Users/cody/text-fabric-data/etcbc/bhsa/tf/c\n",
      "   |     0.23s B number               from /Users/cody/text-fabric-data/etcbc/bhsa/tf/c\n",
      "   |     0.10s B prs                  from /Users/cody/text-fabric-data/etcbc/bhsa/tf/c\n",
      "   |     0.13s B nu                   from /Users/cody/text-fabric-data/etcbc/bhsa/tf/c\n",
      "   |     2.89s B mother               from /Users/cody/text-fabric-data/etcbc/bhsa/tf/c\n",
      "   |     0.10s B st                   from /Users/cody/text-fabric-data/etcbc/bhsa/tf/c\n",
      "   |     0.12s B language             from /Users/cody/text-fabric-data/etcbc/bhsa/tf/c\n",
      "   |     0.11s B ls                   from /Users/cody/text-fabric-data/etcbc/bhsa/tf/c\n",
      "   |     0.17s B rela                 from /Users/cody/text-fabric-data/etcbc/bhsa/tf/c\n",
      "   |     0.04s B obj_prep             from /Users/cody/github/etcbc/heads/tf/c\n",
      "   |     0.03s B sem_set              from /Users/cody/github/etcbc/heads/tf/c\n",
      "   |     0.46s B head                 from /Users/cody/github/etcbc/heads/tf/c\n",
      "   |     0.35s B nhead                from /Users/cody/github/etcbc/heads/tf/c\n",
      "   |     0.00s T note                 from /Users/cody/github/csl/time_collocations/data/tf\n",
      "   |     0.57s T function             from /Users/cody/github/csl/time_collocations/data/tf\n",
      "    12s All features loaded/computed - for details use loadLog()\n",
      "\tconnecting to online GitHub repo annotation/app-bhsa ... connected\n",
      "Using TF-app in /Users/cody/text-fabric-data/annotation/app-bhsa/code:\n",
      "\trv1.0=#d3cf8f0c2ab5d690a0fda14ea31c33da5c5c8483 (latest release)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "@font-face {\n",
       "  font-family: \"Ezra SIL\";\n",
       "  src:\n",
       "    local(\"SILEOT.ttf\"),\n",
       "    url(\"https://github.com/annotation/text-fabric/blob/master/tf/server/static/fonts/SILEOT.woff?raw=true\");\n",
       "}\n",
       ".features {\n",
       "    font-family: monospace;\n",
       "    font-size: medium;\n",
       "    font-weight: bold;\n",
       "    color: #0a6611;\n",
       "    display: flex;\n",
       "    flex-flow: column nowrap;\n",
       "    padding: 0.1em;\n",
       "    margin: 0.1em;\n",
       "    direction: ltr;\n",
       "}\n",
       ".features div,.features span {\n",
       "    padding: 0;\n",
       "    margin: -0.1rem 0;\n",
       "}\n",
       ".features .f {\n",
       "    font-family: sans-serif;\n",
       "    font-size: x-small;\n",
       "    font-weight: normal;\n",
       "    color: #5555bb;\n",
       "}\n",
       ".features .xft {\n",
       "  color: #000000;\n",
       "  background-color: #eeeeee;\n",
       "  font-size: medium;\n",
       "  margin: 0.1em 0em;\n",
       "}\n",
       ".features .xft .f {\n",
       "  color: #000000;\n",
       "  background-color: #eeeeee;\n",
       "  font-style: italic;\n",
       "  font-size: small;\n",
       "  font-weight: normal;\n",
       "}\n",
       ".ltr {\n",
       "    direction: ltr ! important;\n",
       "}\n",
       ".verse {\n",
       "    display: flex;\n",
       "    flex-flow: row wrap;\n",
       "    direction: rtl;\n",
       "}\n",
       ".vl {\n",
       "    display: flex;\n",
       "    flex-flow: column nowrap;\n",
       "    justify-content: flex-end;\n",
       "    align-items: flex-end;\n",
       "    direction: ltr;\n",
       "    width: 100%;\n",
       "}\n",
       ".outeritem {\n",
       "    display: flex;\n",
       "    flex-flow: row wrap;\n",
       "    direction: rtl;\n",
       "}\n",
       ".sentence,.clause,.phrase {\n",
       "    margin-top: -1.2em;\n",
       "    margin-left: 1em;\n",
       "    background: #ffffff none repeat scroll 0 0;\n",
       "    padding: 0 0.3em;\n",
       "    border-style: solid;\n",
       "    border-radius: 0.2em;\n",
       "    font-size: small;\n",
       "    display: block;\n",
       "    width: fit-content;\n",
       "    max-width: fit-content;\n",
       "    direction: ltr;\n",
       "}\n",
       ".atoms {\n",
       "    display: flex;\n",
       "    flex-flow: row wrap;\n",
       "    margin: 0.3em;\n",
       "    padding: 0.3em;\n",
       "    direction: rtl;\n",
       "    background-color: #ffffff;\n",
       "}\n",
       ".satom,.catom,.patom {\n",
       "    margin: 0.3em;\n",
       "    padding: 0.3em;\n",
       "    border-radius: 0.3em;\n",
       "    border-style: solid;\n",
       "    display: flex;\n",
       "    flex-flow: column nowrap;\n",
       "    direction: rtl;\n",
       "    background-color: #ffffff;\n",
       "}\n",
       ".sentence {\n",
       "    border-color: #aa3333;\n",
       "    border-width: 1px;\n",
       "}\n",
       ".clause {\n",
       "    border-color: #aaaa33;\n",
       "    border-width: 1px;\n",
       "}\n",
       ".phrase {\n",
       "    border-color: #33aaaa;\n",
       "    border-width: 1px;\n",
       "}\n",
       ".satom {\n",
       "    border-color: #aa3333;\n",
       "    border-width: 4px;\n",
       "}\n",
       ".catom {\n",
       "    border-color: #aaaa33;\n",
       "    border-width: 3px;\n",
       "}\n",
       ".patom {\n",
       "    border-color: #33aaaa;\n",
       "    border-width: 3px;\n",
       "}\n",
       ".word {\n",
       "    padding: 0.1em;\n",
       "    margin: 0.1em;\n",
       "    border-radius: 0.1em;\n",
       "    border: 1px solid #cccccc;\n",
       "    display: flex;\n",
       "    flex-flow: column nowrap;\n",
       "    direction: rtl;\n",
       "    background-color: #ffffff;\n",
       "}\n",
       ".lextp {\n",
       "    padding: 0.1em;\n",
       "    margin: 0.1em;\n",
       "    border-radius: 0.1em;\n",
       "    border: 2px solid #888888;\n",
       "    width: fit-content;\n",
       "    display: flex;\n",
       "    flex-flow: column nowrap;\n",
       "    direction: rtl;\n",
       "    background-color: #ffffff;\n",
       "}\n",
       ".occs {\n",
       "    font-size: x-small;\n",
       "}\n",
       ".satom.l,.catom.l,.patom.l {\n",
       "    border-left-style: dotted\n",
       "}\n",
       ".satom.r,.catom.r,.patom.r {\n",
       "    border-right-style: dotted\n",
       "}\n",
       ".satom.lno,.catom.lno,.patom.lno {\n",
       "    border-left-style: none\n",
       "}\n",
       ".satom.rno,.catom.rno,.patom.rno {\n",
       "    border-right-style: none\n",
       "}\n",
       ".tr,.tr a:visited,.tr a:link {\n",
       "    font-family: sans-serif;\n",
       "    font-size: large;\n",
       "    color: #000044;\n",
       "    direction: ltr;\n",
       "    text-decoration: none;\n",
       "}\n",
       ".trb,.trb a:visited,.trb a:link {\n",
       "    font-family: sans-serif;\n",
       "    font-size: normal;\n",
       "    direction: ltr;\n",
       "    text-decoration: none;\n",
       "}\n",
       ".prb,.prb a:visited,.prb a:link {\n",
       "    font-family: sans-serif;\n",
       "    font-size: large;\n",
       "    direction: ltr;\n",
       "    text-decoration: none;\n",
       "}\n",
       ".h,.h a:visited,.h a:link {\n",
       "    font-family: \"Ezra SIL\", \"SBL Hebrew\", sans-serif;\n",
       "    font-size: large;\n",
       "    color: #000044;\n",
       "    direction: rtl;\n",
       "    text-decoration: none;\n",
       "}\n",
       ".hb,.hb a:visited,.hb a:link {\n",
       "    font-family: \"Ezra SIL\", \"SBL Hebrew\", sans-serif;\n",
       "    font-size: large;\n",
       "    line-height: 2;\n",
       "    direction: rtl;\n",
       "    text-decoration: none;\n",
       "}\n",
       ".vn {\n",
       "  font-size: small !important;\n",
       "  padding-right: 1em;\n",
       "}\n",
       ".rela,.function,.typ {\n",
       "    font-family: monospace;\n",
       "    font-size: small;\n",
       "    color: #0000bb;\n",
       "}\n",
       ".pdp,.pdp a:visited,.pdp a:link {\n",
       "    font-family: monospace;\n",
       "    font-size: medium;\n",
       "    color: #0000bb;\n",
       "    text-decoration: none;\n",
       "}\n",
       ".voc_lex {\n",
       "    font-family: monospace;\n",
       "    font-size: medium;\n",
       "    color: #0000bb;\n",
       "}\n",
       ".vs {\n",
       "    font-family: monospace;\n",
       "    font-size: medium;\n",
       "    font-weight: bold;\n",
       "    color: #0000bb;\n",
       "}\n",
       ".vt {\n",
       "    font-family: monospace;\n",
       "    font-size: medium;\n",
       "    font-weight: bold;\n",
       "    color: #0000bb;\n",
       "}\n",
       ".gloss {\n",
       "    font-family: sans-serif;\n",
       "    font-size: small;\n",
       "    font-weight: normal;\n",
       "    color: #444444;\n",
       "}\n",
       ".vrs {\n",
       "    font-family: sans-serif;\n",
       "    font-size: small;\n",
       "    font-weight: bold;\n",
       "    color: #444444;\n",
       "}\n",
       ".nd {\n",
       "    font-family: monospace;\n",
       "    font-size: x-small;\n",
       "    color: #999999;\n",
       "}\n",
       ".hl {\n",
       "    background-color: #ffee66;\n",
       "}\n",
       "\n",
       "tr.tf, td.tf, th.tf {\n",
       "  text-align: left;\n",
       "}\n",
       "\n",
       "span.hldot {\n",
       "\tbackground-color: var(--hl-strong);\n",
       "\tborder: 0.2rem solid var(--hl-rim);\n",
       "\tborder-radius: 0.4rem;\n",
       "\t/*\n",
       "\tdisplay: inline-block;\n",
       "\twidth: 0.8rem;\n",
       "\theight: 0.8rem;\n",
       "\t*/\n",
       "}\n",
       "span.hl {\n",
       "\tbackground-color: var(--hl-strong);\n",
       "\tborder-width: 0;\n",
       "\tborder-radius: 0.1rem;\n",
       "\tborder-style: solid;\n",
       "}\n",
       "\n",
       "span.hlup {\n",
       "\tborder-color: var(--hl-dark);\n",
       "\tborder-width: 0.1rem;\n",
       "\tborder-style: solid;\n",
       "\tborder-radius: 0.2rem;\n",
       "  padding: 0.2rem;\n",
       "}\n",
       "\n",
       ":root {\n",
       "\t--hl-strong:        hsla( 60, 100%,  70%, 0.9  );\n",
       "\t--hl-rim:           hsla( 55, 100%,  60%, 0.9  );\n",
       "\t--hl-dark:          hsla( 55, 100%,  40%, 0.9  );\n",
       "}\n",
       "</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import collections, random, csv\n",
    "import pandas as pd\n",
    "from tf.fabric import Fabric\n",
    "from tf.app import use\n",
    "\n",
    "locs = {'bhsa':'~/text-fabric-data/etcbc/bhsa/tf/c',\n",
    "        'heads':'~/github/etcbc/heads/tf/c',\n",
    "        'custom':'~/github/csl/time_collocations/data/tf'}\n",
    "\n",
    "# load BHSA\n",
    "TF = Fabric(locations=locs.values())\n",
    "api = TF.load('''\n",
    "\n",
    "vs vt pdp gloss lex typ number  prs\n",
    "g_cons_utf8 nu mother st language ls rela\n",
    "obj_prep sem_set head nhead\n",
    "note function\n",
    "\n",
    "''')\n",
    "\n",
    "A = use('bhsa', api=api, hoist=globals(), silent=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dicts for object manipulation\n",
    "\n",
    "Writing new TF objects to BHSA requires us to link up new data with old data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodeFeatures=collections.defaultdict(lambda:collections.defaultdict())\n",
    "edgeFeatures=collections.defaultdict(lambda:collections.defaultdict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recreate ETCBC otype, oslots, and function features\n",
    "# these will be appended to rather than altered\n",
    "nodeFeatures['otype'] = dict((n, F.otype.v(n)) for n in N())\n",
    "edgeFeatures['oslots'] = dict((n, L.d(n, 'word')) for n in N() if F.otype.v(n) != 'word')\n",
    "nodeFeatures['function'] = dict((n, F.function.v(n)) for n in F.otype.s('phrase'))\n",
    "nodeFeatures['note'] = dict((n, F.note.v(n)) for n in N() if F.note.v(n))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge Phrases (`phrase2`)\n",
    "\n",
    "Some phrases are unnecessarily split into two. This is fixed by creating a new object, `phrase2`. Later on, another object will be generated, `cx` (construction), which will contain any mixture of single words, phrases, and sentences, essentially ignoring the old, strict divisions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_in_silence = True # whether to report which phrases are being modified"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merge Adjacent TP's\n",
    "\n",
    "There are several cases in BHSA where time phrases are divided up into 2, 3, or even 4 pieces, whereas elsewhere the parts are kept together as a single phrase. This is an undesirable inconsistency. To solve this problem, new phrase boundaries are generated and mapped over the old boundaries stored in the oslots file. For all of the cases that are remapped, a print-out confirms the new slots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0.84s 62 results\n",
      "  0.53s 72 results\n"
     ]
    }
   ],
   "source": [
    "first_tp = set(res[1] for res in A.search('''\n",
    "\n",
    "% find all cases of time phrases followed by, \n",
    "% but not preceded by, another time phrase\n",
    "\n",
    "clause\n",
    "    phrase function=Time\n",
    "    /without/\n",
    "    clause\n",
    "        phrase function=Time\n",
    "        <: ..\n",
    "    /-/\n",
    "    <: phrase function=Time\n",
    "''', silent=False))\n",
    "\n",
    "delete_tp = set(res[2] for res in A.search('''\n",
    "\n",
    "% find all cases of time phrases preceded by\n",
    "% another time phrase for deletion\n",
    "\n",
    "clause\n",
    "    phrase function=Time\n",
    "    <: phrase function=Time\n",
    "\n",
    "''', silent=False))\n",
    "\n",
    "oldmaxotype = max(nodeFeatures['otype'].keys())\n",
    "maxotype = oldmaxotype+1\n",
    "new_phrase = set()\n",
    "\n",
    "for phrase in F.otype.s('phrase'):\n",
    "    \n",
    "    if phrase not in first_tp|delete_tp:\n",
    "        edgeFeatures['oslots'][maxotype] = L.d(phrase, 'word')\n",
    "        nodeFeatures['otype'][maxotype] = 'phrase2'\n",
    "        nodeFeatures['function'][maxotype] = F.function.v(phrase)\n",
    "        maxotype += 1\n",
    "    \n",
    "    elif phrase in first_tp: \n",
    "        new_slots = list(L.d(phrase, 'word')) # compile new slots here\n",
    "        this_phrase, this_clause = phrase, L.u(phrase, 'clause')[0] # this_phrase iterates +1 each loop, this_clause does not\n",
    "        \n",
    "        # gather all slots in subsequent time phrases\n",
    "        while (F.function.v(this_phrase+1) == 'Time')\\\n",
    "            and this_phrase+1 in L.d(this_clause, 'phrase'): # subsequent TP must also be in same clause\n",
    "            new_slots.extend(L.d(this_phrase+1, 'word'))\n",
    "            this_phrase = this_phrase+1\n",
    "\n",
    "        edgeFeatures['oslots'][maxotype] = new_slots\n",
    "        nodeFeatures['otype'][maxotype] = 'phrase2'\n",
    "        nodeFeatures['note'][maxotype] = 'new phrase by phrase merge'\n",
    "        nodeFeatures['function'][maxotype] = 'Time'\n",
    "        new_phrase.add(maxotype)\n",
    "        maxotype+=1\n",
    "        \n",
    "    elif phrase in delete_tp and not merge_in_silence: # delete by skipping\n",
    "        print(f\"skipping over tp {phrase} {T.text(phrase)}\")\n",
    "        print(f\"\\tin {T.text(L.u(phrase,'clause')[0])}\")\n",
    "        print()\n",
    "        continue\n",
    "           \n",
    "if not merge_in_silence:\n",
    "    print('new phrases: ')\n",
    "    for np in sorted(new_phrase):\n",
    "        print(np, T.text(edgeFeatures['oslots'][np]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check for gaps. Every slot should be inside a `phrase2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no problems found!\n"
     ]
    }
   ],
   "source": [
    "coveredslots = sorted(slot for ph, slots in edgeFeatures['oslots'].items()\n",
    "                          for slot in slots\n",
    "                          if ph > oldmaxotype)\n",
    "\n",
    "for i, slot in enumerate(coveredslots):\n",
    "    if i+2 > len(coveredslots):\n",
    "        print('no problems found!')\n",
    "        break\n",
    "    if slot+1 != coveredslots[i+1]:\n",
    "        raise Exception(f'{slot}, {coveredslots[i+1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Complex Cases\n",
    "\n",
    "Some cases will require further research. These are marked with a new feature, called simply a note. The note in this case is \"complex\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodeFeatures['note'][846434] = 'complex'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chunking\n",
    "\n",
    "Building sub-phrase-like chunks based on `heads`.\n",
    "\n",
    "## Noun Chunks\n",
    "\n",
    "**!TODO!**: This section requires more work than I can give to it now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # first handle NP's\n",
    "\n",
    "# np_chunks = collections.defaultdict(list)\n",
    "\n",
    "# for np in F.typ.s('NP'):\n",
    "    \n",
    "#     if F.otype.v(np) == 'phrase_atom':\n",
    "#         continue\n",
    "    \n",
    "#     heads = E.head.t(np)\n",
    "#     ph_words = L.d(np, 'word')\n",
    "    \n",
    "#     # build the chunks\n",
    "#     this_chunk = []\n",
    "#     for i, word in enumerate(ph_words):\n",
    "        \n",
    "#         if F.pdp.v(word) == 'conj': # don't include conjunctions in chunks\n",
    "#             continue\n",
    "        \n",
    "#         if word in heads:\n",
    "#             this_chunk.append(word)\n",
    "#             np_chunks[np].append(this_chunk)\n",
    "#             this_chunk = []\n",
    "            \n",
    "#         elif i == len(ph_words)-1:\n",
    "#             this_chunk.append(word)\n",
    "#             np_chunks[np].append(this_chunk)            \n",
    "#         else:\n",
    "#             this_chunk.append(word)\n",
    "    \n",
    "# len(np_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find NP's of Time with > 1 head and without a cardinal number\n",
    "\n",
    "# A.show(A.search('''\n",
    "\n",
    "# p:phrase typ=NP function=Time\n",
    "# /without/\n",
    "#     word ls=card\n",
    "# /-/\n",
    "# /with/\n",
    "#     t:word pdp#conj\n",
    "#     /without/\n",
    "#     phrase\n",
    "#         <head- t\n",
    "#     /-/\n",
    "#     word pdp=conj\n",
    "# /-/\n",
    "#     w1:word\n",
    "#     < w2:word\n",
    "    \n",
    "# p <head- w1\n",
    "# p <head- w2\n",
    "# '''), end=50, condensed=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantifier Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  2.10s 471 results\n",
      "  2.12s 56 results\n",
      "  2.42s 11 results\n",
      "  2.03s 82 results\n",
      "  1.55s 121 results\n"
     ]
    }
   ],
   "source": [
    "# quantification atoms\n",
    "quant_atoms = []\n",
    "\n",
    "# quant_subs\n",
    "quant_atoms.extend((res[1], res[2]) for res in A.search('''\n",
    "\n",
    "phrase function=Time\n",
    "    word ls=card language=Hebrew\n",
    "    <: word pdp=subs ls#card sem_set#prep\n",
    "\n",
    "'''))\n",
    "\n",
    "# subs_quant\n",
    "quant_atoms.extend((res[2], res[3]) for res in A.search('''\n",
    "\n",
    "phrase function=Time\n",
    "    phrase_atom\n",
    "        word pdp=subs ls#card sem_set#prep st=a language=Hebrew\n",
    "        <: word ls=card\n",
    "\n",
    "'''))\n",
    "\n",
    "# quant_h_subs\n",
    "quant_atoms.extend((res[1], res[3]) for res in A.search('''\n",
    "\n",
    "phrase function=Time\n",
    "    word ls=card language=Hebrew st=c\n",
    "    <: word pdp=art\n",
    "    <: word pdp=subs ls#card\n",
    "\n",
    "'''))\n",
    "\n",
    "# quant_w_quant\n",
    "quant_atoms.extend((res[1], res[3]) for res in A.search('''\n",
    "\n",
    "phrase function=Time\n",
    "    word ls=card language=Hebrew\n",
    "    <: word lex=W\n",
    "    <: word ls=card\n",
    "\n",
    "'''))\n",
    "\n",
    "# quant_quant\n",
    "quant_atoms.extend((res[1], res[2]) for res in A.search('''\n",
    "\n",
    "phrase function=Time\n",
    "    word ls=card language=Hebrew\n",
    "    <: word ls=card\n",
    "\n",
    "'''))\n",
    "\n",
    "quant_atoms.sort()\n",
    "\n",
    "quant_chunks = []\n",
    "\n",
    "i = 0\n",
    "\n",
    "while i < len(quant_atoms):\n",
    "\n",
    "    worda, wordb = quant_atoms[i][0], quant_atoms[i][-1]\n",
    "    nexta, nextb = quant_atoms[i+1][0], quant_atoms[i+1][-1]\n",
    "    qchunk = [worda, wordb]\n",
    "    \n",
    "    while wordb == nexta:\n",
    "        \n",
    "        qchunk.append(nextb)\n",
    "        i += 1\n",
    "        if i == len(quant_atoms)-1: break\n",
    "        worda, wordb = quant_atoms[i][0], quant_atoms[i][-1]\n",
    "        nexta, nextb = quant_atoms[i+1][0], quant_atoms[i+1][-1]\n",
    "        \n",
    "    quant_chunks.append(qchunk)\n",
    "    i += 1\n",
    "\n",
    "def fillGaps(chunk):\n",
    "    '''\n",
    "    Fills in gapped slots such as waws and other\n",
    "    items that are missing in the chunk.\n",
    "    '''\n",
    "    chunk.sort()\n",
    "    minSlot, maxSlot = chunk[0], chunk[-1]\n",
    "    return list(range(minSlot, maxSlot+1))\n",
    "\n",
    "# add quantification construction objects and their features\n",
    "maxNode = max(edgeFeatures['oslots'])+1\n",
    "for chunk in quant_chunks:\n",
    "    node = maxNode\n",
    "    maxNode += 1\n",
    "    nodeFeatures['otype'][node] = 'chunk'\n",
    "    edgeFeatures['oslots'][node] = fillGaps(chunk)\n",
    "    \n",
    "    # map individual semantic roles within construction\n",
    "    quantified_noun = False\n",
    "    for w in chunk:\n",
    "        if F.ls.v(w) != 'card' and F.lex.v(w) != 'H':\n",
    "            edgeFeatures['role'][w] = {node:'quantified'}\n",
    "            quantified_noun = True\n",
    "        elif F.ls.v(w) == 'card':\n",
    "            edgeFeatures['role'][w] = {node:'quantifier'}\n",
    "    \n",
    "    label = 'quant_NP' if quantified_noun else 'quant'\n",
    "    nodeFeatures['label'][node] = label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write cardinal numbers that stand on their own but are not quantified NP's\n",
    "\n",
    "These cases still assume a chained form in Time Phrases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "quant_slots = set(slot for nde in edgeFeatures['oslots'] \n",
    "                     if nodeFeatures['otype'][nde]=='chunk'\n",
    "                     for slot in edgeFeatures['oslots'][nde])\n",
    "\n",
    "non_quant_slots = set(w for w in F.otype.s('word') if w not in quant_slots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1.15s 79 results\n"
     ]
    }
   ],
   "source": [
    "# quant_alone\n",
    "quant_alone = A.search('''\n",
    "\n",
    "phrase function=Time\n",
    "    alonequant ls=card language=Hebrew\n",
    "\n",
    "''', sets={'alonequant':non_quant_slots})\n",
    "\n",
    "for res in quant_alone:\n",
    "    node = maxNode\n",
    "    maxNode += 1\n",
    "    nodeFeatures['otype'][node] = 'chunk'\n",
    "    nodeFeatures['label'][node] = 'quant'\n",
    "    edgeFeatures['oslots'][node] = (res[1],)\n",
    "    edgeFeatures['role'][res[1]] = {node:'quantifier'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chunk Component Quantifier\n",
    "\n",
    "How to handle the following construction?\n",
    "\n",
    "> [ [ numberChain + nounA ] + [ numberChain + nounA ] ]\n",
    "\n",
    "In this construction, where nounA = nounA, the entire construction functions as a single number. This example simultaneously shows how a complex construction can be compiled from smaller component versions. Indeed, both numberChain + noun combinations function as a single constructional unit indicating a quantified noun. But when two of these are used back to back with the same noun, they are to be read together as a single quantified unit. As the outter-most brackets indicate, this itself functions as a construction.\n",
    "\n",
    "These can be found by first mapping the chunks to a phrase node number, and then comparing the identity of the noun."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "phrase2chunks = collections.defaultdict(list)\n",
    "\n",
    "for chunk in quant_chunks:\n",
    "    phrase_atom = L.u(chunk[0], 'phrase_atom')[0]\n",
    "    phrase2chunks[phrase_atom].append(chunk)\n",
    "    \n",
    "# add component chunks to the database\n",
    "for phrase, chunks in phrase2chunks.items():\n",
    "    \n",
    "    if len(chunks) < 2: \n",
    "        continue\n",
    "    \n",
    "    chunknouns = [w for chunk in chunks for w in chunk \n",
    "                      if (F.ls.v(w) != 'card') and (F.lex.v(w) != 'H')]\n",
    "    \n",
    "    if len(chunknouns) < 2:\n",
    "        continue\n",
    "    \n",
    "    nounA, nounB = chunknouns[:2]\n",
    "    \n",
    "    # generate compositive quantitative object\n",
    "    chunk = [w for chunk in chunks for w in chunk]\n",
    "    node = maxNode\n",
    "    maxNode += 1\n",
    "    nodeFeatures['otype'][node] = 'chunk'\n",
    "    nodeFeatures['label'][node] = 'quant_NP_chain'\n",
    "    edgeFeatures['oslots'][node] = fillGaps(chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Preposition Chunks\n",
    "\n",
    "Prepositions that are chained together function as a single directional unit, and some words function as prepositions within a certain frame where elsewhere they may function as nouns. Using the `sem_set` feature from the `heads` project and the `obj_prep` edge relation, we can easily export a construction that can cover these cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def climbPrepChain(prep, prep_list):\n",
    "    '''\n",
    "    Recursively climbs a prepositional chain.\n",
    "    '''\n",
    "    prep_list.append(prep)\n",
    "    daughter = next((po for po in E.obj_prep.t(prep) if F.sem_set.v(po)=='prep'),[])\n",
    "    if daughter:\n",
    "        climbPrepChain(daughter, prep_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "73963 new preposition chunks made...\n"
     ]
    }
   ],
   "source": [
    "new_obj = []\n",
    "\n",
    "for prep in F.sem_set.s('prep'):\n",
    "    \n",
    "    # skip governed preps\n",
    "    if E.obj_prep.f(prep):\n",
    "        continue\n",
    "    \n",
    "    # climb down prep chain\n",
    "    prep_cx = []\n",
    "    climbPrepChain(prep, prep_cx)\n",
    "    \n",
    "    # export object\n",
    "    node = maxNode\n",
    "    new_obj.append(node)\n",
    "    maxNode += 1\n",
    "    nodeFeatures['otype'][node] = 'chunk'\n",
    "    nodeFeatures['label'][node] = 'prep'\n",
    "    edgeFeatures['oslots'][node] = prep_cx\n",
    "    \n",
    "print(len(new_obj), 'new preposition chunks made...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export New Object Data to TF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features to be added...\n",
      "\t otype\n",
      "\t function\n",
      "\t note\n",
      "\t label\n",
      "\t oslots\n",
      "\t role\n"
     ]
    }
   ],
   "source": [
    "print('Features to be added...')\n",
    "for data in (nodeFeatures, edgeFeatures):\n",
    "    for feat in data:\n",
    "        print('\\t', feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is Text-Fabric 7.4.11\n",
      "Api reference : https://annotation.github.io/text-fabric/Api/Fabric/\n",
      "\n",
      "2 features found and 0 ignored\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0.00s Warp feature \"otype\" not found in\n",
      "/Users/cody/github/csl/time_collocations/analysis/../data/\n",
      "  0.00s Warp feature \"oslots\" not found in\n",
      "/Users/cody/github/csl/time_collocations/analysis/../data/\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0.00s Warp feature \"otext\" not found. Working without Text-API\n",
      "\n",
      "  0.00s Exporting 4 node and 2 edge and 1 config features to /Users/cody/github/csl/time_collocations/analysis/../data:\n",
      "  0.00s VALIDATING oslots feature\n",
      "  0.15s maxSlot=     426584\n",
      "  0.15s maxNode=    1774598\n",
      "  0.42s OK: oslots is valid\n",
      "   |     0.71s T function             to /Users/cody/github/csl/time_collocations/analysis/../data\n",
      "   |     0.10s T label                to /Users/cody/github/csl/time_collocations/analysis/../data\n",
      "   |     0.00s T note                 to /Users/cody/github/csl/time_collocations/analysis/../data\n",
      "   |     0.78s T otype                to /Users/cody/github/csl/time_collocations/analysis/../data\n",
      "   |     3.84s T oslots               to /Users/cody/github/csl/time_collocations/analysis/../data\n",
      "   |     0.01s T role                 to /Users/cody/github/csl/time_collocations/analysis/../data\n",
      "   |     0.00s M steps                to /Users/cody/github/csl/time_collocations/analysis/../data\n",
      "  5.86s Exported 4 node features and 2 edge features and 1 config features to /Users/cody/github/csl/time_collocations/analysis/../data\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "featuredir = '../../data'\n",
    "\n",
    "phrasefunctions = dict((ph, F.function.v(ph)) for ph in F.otype.s('phrase'))\n",
    "\n",
    "meta = {'':{'source': 'https://github.com/etcbc/bhsa',\n",
    "            'origin': 'Made by the ETCBC of the Vrije Universiteit Amsterdam; edited by Cody Kingham',\n",
    "            'coreData':'BHSA',\n",
    "            'coreVersion':'c',},\n",
    "        'oslots': {'valueType':'int', \n",
    "                   'edgeValues':False},\n",
    "        'otype':{'valueType':'str'},\n",
    "        'note':{'valueType':'str',\n",
    "                'description':'notes on objects for tracking issues throughout my research'},\n",
    "        'role':{'edgeValues':True,\n",
    "                'valueType':'str', \n",
    "                'description':'role of the word in the chunk'},\n",
    "        'label':{'valueType':'str'},\n",
    "        'steps':{'valueType':'int'},\n",
    "        'function':{'valueType':'str'}\n",
    "       }\n",
    "\n",
    "TFsave = Fabric(locations=featuredir)\n",
    "\n",
    "TFsave.save(nodeFeatures=nodeFeatures, edgeFeatures=edgeFeatures, metaData=meta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring New Objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "locations2 = ['/Users/cody/text-fabric-data/etcbc/bhsa/tf/c/',\n",
    "             '../../data/',]\n",
    "\n",
    "TF2 = Fabric(locations=locations2)\n",
    "api2 = TF2.load('''\n",
    "\n",
    "vs vt pdp gloss lex \n",
    "language rela typ number\n",
    "function\n",
    "role label\n",
    "''')\n",
    "\n",
    "B = use('bhsa', api=api2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "B.show(B.search('''\n",
    "\n",
    "phrase function=Time\n",
    "    chunk label=prep\n",
    "        word\n",
    "        < word\n",
    "\n",
    "'''), condenseType='clause', condensed=True, end=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenPhrase(phrasenode):\n",
    "    '''Tokenizes a phrase with\n",
    "    dot-separated words.\n",
    "    input: phrase node number\n",
    "    output: token string'''\n",
    "    words = [(F.g_cons_utf8.v(w) if F.lex.v(w) != 'H' else 'ה') for w in L.d(phrasenode, 'word')]\n",
    "    return '.'.join(words)\n",
    "\n",
    "def tokenHeads(headslist):\n",
    "    '''same as tokenPhrase but with list of head word nodes'''\n",
    "    return '.'.join((F.g_cons_utf8.v(w) if F.lex.v(w) != 'H' else 'ה') for w in headslist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phrase Tokens and Phrase Heads\n",
    "\n",
    "This search counts all of the discrete time phrase tokens in Hebrew and gathers data about their heads. This data is exported to a spreadsheet for manual inspection. Per every token, a key of its heads is saved into a dictionary, linked to a list of phrase nodes. Tokens that have more than 1 head are suspicious, since the surface form is the same. All other tokens will be exported with their standard heads for inspection. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tp_heads = collections.defaultdict(lambda: collections.defaultdict(list))\n",
    "tp_nheads = collections.defaultdict(lambda: collections.defaultdict(list))\n",
    "tp_count = collections.Counter()\n",
    "\n",
    "tps = A.search('''\n",
    "\n",
    "phrase function=Time\n",
    "/with/\n",
    "    word language=Hebrew\n",
    "/-/\n",
    "\n",
    "''', shallow=True)\n",
    "\n",
    "for tp in tps:\n",
    "    token = tokenPhrase(tp)\n",
    "    heads_token = tokenHeads(E.head.t(tp))\n",
    "    nheads_token = tokenHeads(E.nhead.t(tp))\n",
    "    \n",
    "    \n",
    "    tp_heads[token][heads_token].append(tp)\n",
    "        \n",
    "    # only populate nheads with PP phrases, since nhead feature for NP is exactly the same\n",
    "    if F.typ.v(tp) == 'PP':\n",
    "        tp_nheads[token][nheads_token].append(tp)\n",
    "        \n",
    "    tp_count[token] += 1\n",
    "    \n",
    "suspect_heads = [tp for tp in tp_heads if len(tp_heads[tp]) > 1]\n",
    "suspect_nheads = [tp for tp in tp_nheads if len(tp_nheads[tp]) > 1]\n",
    "\n",
    "print(f'total phrase tokens 2 head mappings: {len(tp_heads)}')\n",
    "print(f'total phrase tokens 2 nhead mappings: {len(tp_nheads)}')\n",
    "print(f'total suspect heads: {len(suspect_heads)}')\n",
    "print(f'total suspect nheads {len(suspect_nheads)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NB**<br>\n",
    "The initial run of this search found problems in the phrase: ב.ה.בקר.ב.ה.בקר. Some cases marked the second part of the phrase a parallel element, whereas others marked them as either a phrase atom specification relation (`Spec`) or a subphrase adjunct relation (`adj`). This is an inconsistent tagging on the BHSA's part. These issues were addressed in the [heads notebook](https://nbviewer.jupyter.org/github/ETCBC/heads/blob/master/phrase_heads.ipynb) of the ETCBC heads repository. The phrase in question is now correctly annotated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile Manual Inspection Spreadsheet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tp_heads_data = []\n",
    "# tp_nheads_data = []\n",
    "# data_header = ['token', '(n)heads_token', 'freq', 'mark', 'note', 'ex_ref', 'ex', 'ex_node', 'ex_verse']\n",
    "\n",
    "# for htp, nhtp in zip(tp_heads.keys(), tp_nheads.keys()):\n",
    "#     head = next(tp for tp in tp_heads[htp])\n",
    "#     nhead = next(tp for tp in tp_nheads[nhtp])\n",
    "#     head_ex = random.choice(tp_heads[htp][head])\n",
    "#     nhead_ex = random.choice(tp_nheads[nhtp][nhead])\n",
    "    \n",
    "#     head_ref, nhead_ref = ['{} {}:{}'.format(*T.sectionFromNode(ex)) for ex in (head_ex, nhead_ex)]\n",
    "#     head_txt, nhead_txt = [T.text(ex) for ex in (head_ex, nhead_ex)]\n",
    "#     head_verse, nhead_verse = [T.text(L.u(ex, 'verse')[0]) for ex in (head_ex, nhead_ex)]\n",
    "    \n",
    "#     heads_data = [htp, head, tp_count[htp], '', '', head_ref, head_txt, head_ex, head_verse]\n",
    "#     nheads_data = [nhtp, nhead, tp_count[nhtp], '', '', nhead_ref, nhead_txt, nhead_ex, nhead_verse]\n",
    "#     tp_heads_data.append(heads_data)\n",
    "#     tp_nheads_data.append(nheads_data)\n",
    "    \n",
    "# tp_heads_data, tp_nheads_data = sorted(tp_heads_data), sorted(tp_nheads_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('manual_curation/tp_heads.csv', 'w') as outfile:\n",
    "#     writer = csv.writer(outfile)\n",
    "#     writer.writerow(data_header)\n",
    "#     writer.writerows(tp_heads_data)\n",
    "    \n",
    "# with open('manual_curation/tp_nheads.csv', 'w') as outfile:\n",
    "#     writer = csv.writer(outfile)\n",
    "#     writer.writerow(data_header)\n",
    "#     writer.writerows(tp_nheads_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manual Review of Heads and Features\n",
    "\n",
    "The manual annotations are intended to serve 2 roles: 1. to evaluate the accuracy of the head assignments on time phrases in the BHSA data, 2. to evaluate the time phrase structure in the BHSA dataset, and 3. to gain hands-on exposure to the kinds of time phrases in the dataset. This process consisted of comparing the selected heads against the surface text of the time phrase, and of reading the time phrases in the context of a verse when questions or anomalies arose. The annotation process consisted of marking a given time phrase as \"g\" for \"good,\" \"b\" for \"bad,\" and \"?\" for questionable cases. These classifications refer to both head assignments and internal structuring of the time phrases in the BHSA. The markings are often accompanied with notes: for bad or questionable entries the note explains what is wrong, for good entries the note might describe an interesting phenomenon, in some cases it might give a \"light caution\" about a given phrase. \n",
    "\n",
    "The annotations suggest that custom database is necessary to consistently represent time phrases: there are many cases in BHSA time phrases where the phrase is cleft into 2 adjacent parts whereas in the majority of the data they are kept together. This is an inconsistency that should be solved. In other cases, the notion of \"phrase\" is not broad enough to encompass the full range of expressions that can mark time. For instance, several time phrases are split off from the infinitives they direct, in which the infinitive is an event. This is because the ETCBC's strict structuralist methodology has defined the infinitive event as a clause, operating at a different hierarchical level than the phrase; phrases are defined as strictly non-predicative. In the framework of Construction Grammar (Goldberg, *Constructions*, 1995, Croft, *Radical Construction Grammar*, 2001) preferred by this study, these divisions are not necessary, and in fact hinder an accurate and comprehensive description. In Construction Grammar, no division is assumed between syntax and semantics, and thus the difference between a clause and a phrase is merely a difference in degree based on the two construction's forms and meanings, but it is not a fundamental difference in kind. Indeed, many phrases in this dataset refer to event-like nouns, which are from a syntactical perspective non-predicative; במות \"in the death of\" is a very frequent example, but other cases include ביום ישׁועה \"in the day of salvation,\" which assumes a salvation event. If the difference between time phrases and time \"clauses\" is seen as merely an incremental difference rather than categorical, then one can apply a unified strategy in analyzing these cases. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', 0)  # configure DataFrame to show full notes with no truncation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "head_anno = pd.read_csv('manual_curation/tp_heads_annotated.csv')\n",
    "nhead_anno = pd.read_csv('manual_curation/tp_nheads_annotated.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Looking at Bad Cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#head_anno[head_anno['mark'] == 'b'].to_csv('manual_curation/head_fixes.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nhead_anno[nhead_anno['mark']=='b'].to_csv('manual_curation/nhead_fixes.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**All situations have been remedied. The chosen solution is stored under the new column, \"fix\", in `head_fixes.csv` and `nhead_fixes.csv`.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Looking at Questionable Cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#head_anno[head_anno['mark'] == '?']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nhead_anno[nhead_anno['mark']=='?']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
