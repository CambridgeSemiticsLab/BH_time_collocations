{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "14bf8e61-97b1-4303-a872-e98e9c84a058",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import collections\n",
    "import shutil\n",
    "\n",
    "from copy import deepcopy\n",
    "from tf.fabric import Fabric\n",
    "from tf.core.api import Api\n",
    "from tf.convert.walker import CV\n",
    "from pathlib import Path\n",
    "from functools import cmp_to_key\n",
    "from typing import Dict, Tuple, Union, Optional, Set, List, Callable, TypedDict, NamedTuple"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b960954-524e-47c3-8fa5-c38f2840b2b0",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "67ab5b74-90b3-423b-a351-a887198ce5c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "BHSA_CORE_DATA = Path('/Users/cody/github/etcbc/bhsa')\n",
    "BHSA_TF = BHSA_CORE_DATA / 'tf/2021'\n",
    "BHSA_YAML = BHSA_CORE_DATA / 'yaml'\n",
    "BHSA_METADATA_FILES = ['core.yaml', 'lexicon.yaml', 'ketivqere.yaml', 'paragraph.yaml', 'stats.yaml']\n",
    "BHSA_METADATA_PATHS = [BHSA_YAML / file for file in BHSA_METADATA_FILES]\n",
    "\n",
    "BHSA_GENERIC = BHSA_YAML / 'generic.yaml'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "c47272b0-aac9-4d89-b39f-be4ff51c16d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_yaml(filepath):\n",
    "    \"\"\"Load yaml config as dict.\"\"\"\n",
    "    with open(filepath, 'r') as infile:\n",
    "        return yaml.load(infile, Loader=yaml.FullLoader)\n",
    "    \n",
    "\n",
    "def load_all_feature_metadata(feature_metadata_paths):\n",
    "    \"\"\"Load all feature metadata into a single dictionary.\"\"\"\n",
    "    return {\n",
    "        feature: value\n",
    "        for path in feature_metadata_paths\n",
    "        for feature, value in load_yaml(path).items()\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "0fbc2751-415a-47da-8d7e-bebfbcc406e1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1.76s Feature overview: 109 for nodes; 6 for edges; 1 configs; 9 computed\n"
     ]
    }
   ],
   "source": [
    "tf_bhsa = Fabric('/Users/cody/github/etcbc/bhsa/tf/2021')\n",
    "bhsa = tf_bhsa.loadAll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "793bedd4-3688-4b7b-b174-2075e0da5949",
   "metadata": {},
   "outputs": [],
   "source": [
    "verseless_clauses = []\n",
    "for cl in bhsa.F.otype.s('clause'):\n",
    "    if not bhsa.L.u(cl, 'verse'):\n",
    "        verseless_clauses.append(cl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "65134400-4016-40b8-8f76-562741b66ff9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(verseless_clauses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "3d6ec5ff-e47d-4224-b746-06ad83e1a481",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "428158"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "verseless_clauses[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "cccbaedd-3557-429c-9418-5cf0f4529780",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'מִן־הַבְּהֵמָה֙ הַטְּהֹורָ֔ה וּמִן־הַ֨בְּהֵמָ֔ה וּמִ֨ן־הָעֹ֔וף וְכֹ֥ל שְׁנַ֨יִם שְׁנַ֜יִם בָּ֧אוּ אֶל־נֹ֛חַ אֶל־הַתֵּבָ֖ה זָכָ֣ר וּנְקֵבָ֑ה '"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bhsa.T.text(428158)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "afe83ea7-fa80-49e8-b4b0-72845678fed8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Genesis', 7, 8)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bhsa.T.sectionFromNode(428158)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f334c855-0ce7-4b30-bcbd-fd73359c2886",
   "metadata": {},
   "outputs": [],
   "source": [
    "BHSAKT_METADATA = {\n",
    "    \"corpus\": \"BHSA-Kingham-thesis\",\n",
    "    \"description\": \"A modified version of the ETCBC's BHSA for my Cambridge PhD thesis\",\n",
    "    \"version\": \"1.0\",\n",
    "    \"editor\": \"Cody Kingham\",\n",
    "    \"source\": \"Eep Talstra Centre for Bible and Computer\",\n",
    "    \"source-url\": \"https://github.com/etcbc/bhsa\",\n",
    "    \"encoders\": \"Constantijn Sikkel (QDF), Ulrik Petersen (MQL) and Dirk Roorda (TF)\",\n",
    "}\n",
    "\n",
    "\n",
    "GENERIC_META = load_yaml(BHSA_GENERIC)\n",
    "GENERIC_META.update({\n",
    "    'dateWritten': None,\n",
    "    'writtenBy': None,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "884aea51-b502-41ee-b1c0-2cf65455a0f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dataset': 'BHSA',\n",
       " 'datasetName': 'Biblia Hebraica Stuttgartensia Amstelodamensis',\n",
       " 'author': 'Eep Talstra Centre for Bible and Computer',\n",
       " 'encoders': 'Constantijn Sikkel (QDF), Ulrik Petersen (MQL) and Dirk Roorda (TF)',\n",
       " 'website': 'https://shebanq.ancient-data.org',\n",
       " 'email': 'shebanq@ancient-data.org',\n",
       " 'dateWritten': None,\n",
       " 'writtenBy': None}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GENERIC_META"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7ddc0f73-a54b-439a-883c-a81f3b4d5130",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_chunk(node):\n",
    "    slots = bhsa.L.d(node, 'word')\n",
    "    return (node, set(slots))\n",
    "\n",
    "\n",
    "def _canonical_order(node_chunk_a, node_chunk_b):\n",
    "    \"\"\"Sort items in canonical sorting order.\"\"\"\n",
    "    na, prec_a, slotsA = node_chunk_a\n",
    "    nb, prec_b, slotsB = node_chunk_b\n",
    "    \n",
    "    # compare based on node precedence\n",
    "    if prec_a > prec_b:\n",
    "        return -1\n",
    "    elif prec_b > prec_a:\n",
    "        return 1\n",
    "    \n",
    "    # compare based on slots\n",
    "    else:\n",
    "        # slots are equivalent\n",
    "        if slotsA == slotsB:\n",
    "            return 0\n",
    "\n",
    "        # a is subset of b\n",
    "        aWithoutB = slotsA - slotsB\n",
    "        if not aWithoutB:\n",
    "            return 1\n",
    "\n",
    "        # b is subset of a\n",
    "        bWithoutA = slotsB - slotsA\n",
    "        if not bWithoutA:\n",
    "            return -1\n",
    "\n",
    "        # compare based on slots\n",
    "        aMin = min(aWithoutB)\n",
    "        bMin = min(bWithoutA)\n",
    "        return -1 if aMin < bMin else 1\n",
    "\n",
    "\n",
    "canonical_order = cmp_to_key(_canonical_order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6f28db59-754f-4f56-b9f9-634a588a2ee2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(2, 10, {1, 2, 3}),\n",
       " (1, 9, {1, 2, 3}),\n",
       " (3, 6, {1, 2, 3}),\n",
       " (3, 6, {1, 2}),\n",
       " (3, 5, {1, 2}),\n",
       " (3, 5, {3, 4})]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test the canonical sorting\n",
    "\n",
    "test = [\n",
    "    (1, 9, {1, 2, 3}),\n",
    "    (2, 10, {1, 2, 3}),\n",
    "    (3, 5, {3, 4}),\n",
    "    (3, 5, {1, 2}),\n",
    "    (3, 6, {1, 2, 3}),\n",
    "    (3, 6, {1, 2})\n",
    "]\n",
    "\n",
    "sorted(test, key=canonical_order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d655417b-80a3-4ce4-bd56-b0af3a1458f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _copy_meta_dicts(feature_dict):\n",
    "    \"\"\"Extract metakwargs.\"\"\"\n",
    "    return {\n",
    "        feat: deepcopy(feat_obj.meta)\n",
    "        for feat, feat_obj in feature_dict.items()\n",
    "    }\n",
    "\n",
    "\n",
    "def _copy_feature_dicts(feature_dict):\n",
    "    \"\"\"Extract feature dicts.\"\"\"\n",
    "    return {\n",
    "        feat: dict(feat_obj.items())\n",
    "        for feat, feat_obj in feature_dict.items()\n",
    "    }\n",
    "\n",
    "    \n",
    "def get_copy_of_corpus(tf_fabric: Fabric):\n",
    "    \"\"\"Get a copy of a corpus's resources.\"\"\"\n",
    "    tf_api = tf_fabric.api\n",
    "    node_features = _copy_feature_dicts(tf_api.F.__dict__)\n",
    "    edge_features = _copy_feature_dicts(tf_api.E.__dict__)\n",
    "    metadata = {\n",
    "        **_copy_meta_dicts(tf_api.F.__dict__),\n",
    "        **_copy_meta_dicts(tf_api.E.__dict__),\n",
    "    }\n",
    "    metadata['otext'] = tf_fabric.features['otext'].metaData\n",
    "    return {\n",
    "        'nodeFeatures': node_features,\n",
    "        'edgeFeatures': edge_features,\n",
    "        'metaData': metadata,\n",
    "    }\n",
    "\n",
    "\n",
    "# add types\n",
    "featureType = Union[str, int]\n",
    "edgeType = Union[Set[int], Dict[int, featureType]]\n",
    "nodeFeatureDict = Dict[str, Dict[int, featureType]]\n",
    "edgeFeatureDict = Dict[str, Dict[int, edgeType]]\n",
    "metaDataDict = Dict[str, Dict[str, str]]\n",
    "\n",
    "class corpusData(TypedDict):\n",
    "    nodeFeature: nodeFeatureDict\n",
    "    edgeFeature: edgeFeatureDict\n",
    "    metaData: metaDataDict\n",
    "\n",
    "featureGenerator = Callable[corpusData, nodeFeatureDict]\n",
    "\n",
    "\n",
    "class editAction:\n",
    "    \"\"\"TypedDict for grouping related corpus edits.\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "            self,\n",
    "            deletions: Optional[Set[int]] = None,\n",
    "            feature_updates: Optional[nodeFeatureDict] = None,\n",
    "            edge_updates: Optional[edgeFeatureDict] = None,\n",
    "    ):\n",
    "        \"\"\"Initialize edit action object.\"\"\"\n",
    "        self.deletions = deletions or set()\n",
    "        self.feature_updates = feature_updates or {}\n",
    "        self.edge_updates = edge_updates or {}\n",
    "\n",
    "\n",
    "class ThesisCorpusBuilder:\n",
    "    \"\"\"Class for building thesis corpus.\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self, \n",
    "        locations: Optional[List[str]] = None,\n",
    "        book_limit: Optional[int] = None,\n",
    "        delete_features: Optional[Set[str]] = None,\n",
    "        rename_features: Optional[Dict[str, str]] = None,\n",
    "        add_features: Optional[Dict[str, featureGenerator]] = None,\n",
    "        update_metadata: Optional[metaDataDict] = None,\n",
    "        update_features: Optional[nodeFeatureDict] = None,\n",
    "        update_edges: Optional[edgeFeatureDict] = None,\n",
    "        delete_nodes: Optional[Set[int]] = None,\n",
    "        edit_actions: Optional[List[editAction]] = None,\n",
    "        tf_fabric: Optional[Fabric] = None,\n",
    "    ):\n",
    "        \"\"\"Initialize the thesis corpus builder.\"\"\"\n",
    "        self.locations = locations or ''\n",
    "        self.book_limit = book_limit\n",
    "        self.add_features = add_features or {}\n",
    "        self.update_metadata = update_metadata or {}\n",
    "        self.delete_features = delete_features or set()\n",
    "        self.rename_features = rename_features or {}\n",
    "        self.delete_nodes = delete_nodes or set()\n",
    "        self.update_features = update_features or {}\n",
    "        self.update_edges = update_edges or {}\n",
    "        self._add_edit_actions(\n",
    "            edit_actions or [],\n",
    "            self.delete_nodes,\n",
    "            self.update_features,\n",
    "            self.update_edges,\n",
    "        )\n",
    "        self.tf_fabric = tf_fabric\n",
    "        self.tf_api = tf_fabric.api if tf_fabric else None\n",
    "\n",
    "    @staticmethod\n",
    "    def _add_edit_actions(\n",
    "            edit_actions: List[editAction],\n",
    "            delete_nodes: Set[int],\n",
    "            update_features: nodeFeatureDict,\n",
    "            update_edges: edgeFeatureDict,\n",
    "    ) -> None:\n",
    "        \"\"\"Add all edit actions to the correct dicts / sets.\"\"\"\n",
    "        for action in edit_actions:\n",
    "            delete_nodes.update(action.deletions)\n",
    "            update_features.update(action.feature_updates)\n",
    "            update_edges.update(action.edge_updates)\n",
    "        \n",
    "    def _get_keep_node_set(self):\n",
    "        \"\"\"Get set of nodes to keep.\"\"\"\n",
    "        keep_nodes = set()\n",
    "        max_slot = 0\n",
    "        book_limit = (\n",
    "            self.tf_api.T.nodeFromSection((self.book_limit,))\n",
    "            if self.book_limit else None\n",
    "        )\n",
    "        for book_node in self.tf_api.F.otype.s('book'):\n",
    "            if book_limit and book_node > book_limit:\n",
    "                break\n",
    "            keep_nodes.add(book_node)\n",
    "            for node in self.tf_api.L.d(book_node):\n",
    "                keep_nodes.add(node)\n",
    "                if self.tf_api.F.otype.v(node) == 'word':\n",
    "                    max_slot = node\n",
    "        return keep_nodes, max_slot\n",
    "    \n",
    "    @staticmethod\n",
    "    def _filter_nodes_from_feature_dict(feature_dict, keep_nodes):\n",
    "        \"\"\"Filter keep nodes.\"\"\"\n",
    "        filtered_feature_dict = {}\n",
    "        for feature, node_dict in feature_dict.items():\n",
    "            filtered_feature_dict[feature] = {\n",
    "                node: feature\n",
    "                for node, feature in node_dict.items()\n",
    "                if node in keep_nodes\n",
    "            }\n",
    "        return filtered_feature_dict\n",
    "    \n",
    "    def _filter_feature_dict_nodes(self, corpus_data, keep_node_set):\n",
    "        \"\"\"Filter feature dict nodes.\"\"\"\n",
    "        corpus_data['nodeFeatures'] = self._filter_nodes_from_feature_dict(\n",
    "            corpus_data['nodeFeatures'],\n",
    "            keep_node_set,\n",
    "        )\n",
    "        corpus_data['edgeFeatures'] = self._filter_nodes_from_feature_dict(\n",
    "            corpus_data['edgeFeatures'],\n",
    "            keep_node_set\n",
    "        )\n",
    "    \n",
    "    def _rebuild_nodes_from_oslots(\n",
    "            self, \n",
    "            oslot_map,\n",
    "            otype_map,\n",
    "            max_slot,\n",
    "    ) -> Dict[int, int]:\n",
    "        \"\"\"Rebuild node numbering scheme from oslots.\"\"\"\n",
    "        # get sorted list of oslot data\n",
    "        oslots = []\n",
    "        for node, oslot_set in oslot_map.items():\n",
    "            otype = otype_map[node]\n",
    "            otype_rank = self.tf_api.Nodes.otypeRank[otype]\n",
    "            oslots.append((node, otype_rank, set(oslot_set)))\n",
    "        oslots.sort(key=canonical_order)\n",
    "        \n",
    "        # create mapping to new node numbers\n",
    "        new_node_map = {\n",
    "            old_node: (i + max_slot)\n",
    "            for i, (old_node, _, _) in enumerate(oslots, 1)\n",
    "        }\n",
    "        return new_node_map\n",
    "        \n",
    "    @staticmethod\n",
    "    def _reindex_node_features(old_node_features, remapper):\n",
    "        \"\"\"Reindex node features.\"\"\"\n",
    "        node_features = collections.defaultdict(dict)\n",
    "        for feature, node_dict in old_node_features.items():\n",
    "            for node, fvalue in node_dict.items():\n",
    "                node_features[feature][remapper(node)] = fvalue\n",
    "        return node_features\n",
    "    \n",
    "    @staticmethod\n",
    "    def _reindex_edge_features(old_edge_features, remapper):\n",
    "        \"\"\"Reindex edge features.\"\"\"\n",
    "        edge_features = collections.defaultdict(dict)\n",
    "        for feature, edge_dict in old_edge_features.items():\n",
    "            for node, edges in edge_dict.items():\n",
    "                if isinstance(edges, dict):\n",
    "                    edge_features[feature][remapper(node)] = {\n",
    "                        remapper(n) for n, v\n",
    "                        in edges.items()\n",
    "                    }\n",
    "                else:\n",
    "                    edge_features[feature][remapper(node)] = set(\n",
    "                        remapper(n) for n in edges\n",
    "                    )\n",
    "        return edge_features\n",
    "        \n",
    "    def _reindex_nodes(self, corpus_data: corpusData, max_slot):\n",
    "        \"\"\"Reindex nodes.\"\"\"\n",
    "        # rebuild node numbering from oslot data\n",
    "        new_node_map = self._rebuild_nodes_from_oslots(\n",
    "            corpus_data['edgeFeatures']['oslots'],\n",
    "            corpus_data['nodeFeatures']['otype'],\n",
    "            max_slot,\n",
    "        )\n",
    "        \n",
    "        # remap all node features using the new numbering scheme\n",
    "        remapper = lambda node: new_node_map.get(node, node)\n",
    "        node_features = self._reindex_node_features(corpus_data['nodeFeatures'], remapper)\n",
    "        edge_features = self._reindex_edge_features(corpus_data['edgeFeatures'], remapper)\n",
    "\n",
    "        # apply the changes to the dict\n",
    "        corpus_data['nodeFeatures'] = node_features\n",
    "        corpus_data['edgeFeatures'] = edge_features\n",
    "        \n",
    "        # add a helper map back to old nodes for referencing\n",
    "        corpus_data['edgeFeatures']['omap@2021-KT'] = {\n",
    "            new_node: {old_node: None}\n",
    "            for old_node, new_node in new_node_map.items()\n",
    "        }\n",
    "\n",
    "    def _update_metadata(self, corpus_data: corpusData):\n",
    "        \"\"\"Update metadata fields.\"\"\"\n",
    "        for field, data in self.update_metadata.items():\n",
    "            corpus_data['metaData'].setdefault(field, {}).update(data)\n",
    "        \n",
    "    def _rebuild_metadata(self, corpus_data: corpusData):\n",
    "        \"\"\"Remap metadata for this project.\"\"\"\n",
    "        new_metadata = {}\n",
    "        for feature, meta in corpus_data['metaData'].items():\n",
    "            unique_meta = {\n",
    "                k: v for k, v in meta.items()\n",
    "                if k not in GENERIC_META\n",
    "            }\n",
    "            new_metadata[feature] = {\n",
    "                **BHSAKT_METADATA,\n",
    "                **unique_meta,\n",
    "            }\n",
    "        corpus_data['metaData'] = new_metadata\n",
    "    \n",
    "    def _delete_features(self, corpus_data: corpusData):\n",
    "        \"\"\"Remove features from the dataset.\"\"\"\n",
    "        for feature in self.delete_features:\n",
    "            for data_type, data_dict in corpus_data.items():                \n",
    "                if feature in data_dict:\n",
    "                    del data_dict[feature]\n",
    "\n",
    "    def _rename_features(self, corpus_data: corpusData):\n",
    "        \"\"\"Rename features in the dataset.\"\"\"\n",
    "        for old_name, new_name in self.rename_features.items():\n",
    "            for data_type, data_dict in corpus_data.items():\n",
    "                if old_name in data_dict:    \n",
    "                    data_dict[new_name] = data_dict[old_name]\n",
    "                    del data_dict[old_name]\n",
    "\n",
    "    def _update_features(self, corpus_data: corpusData):\n",
    "        \"\"\"Update feature node mappings.\"\"\"\n",
    "        for feature, update_dict in self.update_features.items():\n",
    "            corpus_data['nodeFeatures'].setdefault(feature, {}).update(update_dict)\n",
    "\n",
    "    def _update_edges(self, corpus_data: corpusData):\n",
    "        \"\"\"Update edge feature node mappings.\"\"\"\n",
    "        for feature, update_dict in self.update_edges.items():\n",
    "            corpus_data['edgeFeatures'].setdefault(feature, {}).update(update_dict)\n",
    "\n",
    "    def _delete_nodes(self, corpus_data: corpusData):\n",
    "        \"\"\"Delete nodes from the corpus.\"\"\"\n",
    "        # delete from feature values\n",
    "        for feature, node_data in corpus_data['nodeFeatures'].items():\n",
    "            corpus_data['nodeFeatures'][feature] = {\n",
    "                node: value\n",
    "                for node, value in node_data.items()\n",
    "                if node not in self.delete_nodes\n",
    "            }\n",
    "        # delete from edge relations\n",
    "        new_edges = collections.defaultdict(dict)\n",
    "        for feature, edge_data in corpus_data['edgeFeatures'].items():\n",
    "            for node, edges in edge_data.items():\n",
    "                if node in self.delete_nodes:\n",
    "                    continue\n",
    "                elif isinstance(edges, dict):\n",
    "                    new_edges[feature][node] = {\n",
    "                        n: value\n",
    "                        for n, value in edges.items()\n",
    "                        if n not in self.delete_nodes\n",
    "                    }\n",
    "                else:\n",
    "                    new_edges[feature][node] = set(\n",
    "                        n for n in edges\n",
    "                        if n not in self.delete_nodes\n",
    "                    )\n",
    "        corpus_data['edgeFeatures'] = new_edges\n",
    "\n",
    "    @staticmethod\n",
    "    def _clear_directory(dest_dir: str):\n",
    "        \"\"\"Empty a destination directory of old data.\"\"\"\n",
    "        shutil.rmtree(dest_dir)\n",
    "        Path(dest_dir).mkdir(parents=True)\n",
    "            \n",
    "    def _load_tf_corpus(self):\n",
    "        \"\"\"Load Text Fabric corpus.\"\"\"\n",
    "        if not self.tf_fabric:\n",
    "            self.tf_fabric = Fabric(self.locations)\n",
    "            self.tf_api = self.tf_fabric.loadAll()\n",
    "                    \n",
    "    def build(self, dest_dir: str):\n",
    "        \"\"\"Build the corpus.\"\"\"\n",
    "        print('Loading TF corpus...')\n",
    "        self._load_tf_corpus()\n",
    "        \n",
    "        print('Getting a copy of the corpus...')\n",
    "        corpus_data = get_copy_of_corpus(self.tf_fabric)\n",
    "        \n",
    "        print('Filtering the nodes...')\n",
    "        keep_node_set, max_slot = self._get_keep_node_set()\n",
    "        self._filter_feature_dict_nodes(corpus_data, keep_node_set)\n",
    "\n",
    "        print('Applying graph edits...')\n",
    "        self._delete_nodes(corpus_data)\n",
    "        self._update_features(corpus_data)\n",
    "        self._update_edges(corpus_data)\n",
    "\n",
    "        print('Reindexing the nodes...')\n",
    "        self._reindex_nodes(corpus_data, max_slot)\n",
    "        \n",
    "        print('Rebuilding metadata...')\n",
    "        self._update_metadata(corpus_data)\n",
    "        self._rebuild_metadata(corpus_data)\n",
    "        \n",
    "        print('Refactoring features...')\n",
    "        self._delete_features(corpus_data)\n",
    "        self._rename_features(corpus_data)\n",
    "        \n",
    "        print('Saving new corpus...')\n",
    "        self._clear_directory(dest_dir)\n",
    "        saver = Fabric(dest_dir)\n",
    "        saver.save(**corpus_data)\n",
    "        return corpus_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "0cd001d0-8951-4b50-a102-54ec41e39a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "book_limit = '2_Kings'\n",
    "\n",
    "delete_features = {\n",
    "    'book@am', 'book@ar', 'book@bn', 'book@da',\n",
    "    'book@de', 'book@el', 'book@es', 'book@fa', \n",
    "    'book@fr', 'book@he', 'book@hi', 'book@id', \n",
    "    'book@ja', 'book@ko', 'book@la', 'book@nl', \n",
    "    'book@pa', 'book@pt', 'book@ru', 'book@sw', \n",
    "    'book@syc', 'book@tr', 'book@ur', 'book@yo', \n",
    "    'book@zh', 'book', 'dist', 'dist_unit', \n",
    "    'mother_object_type', 'number',\n",
    "    'functional_parent', 'distributional_parent',\n",
    "    'language', 'languageISO',\n",
    "    'omap@c-KT', 'omap@c-2021', 'omap@2017-2021',\n",
    "}\n",
    "\n",
    "rename_features = {\n",
    "    'book@en': 'book',\n",
    "}\n",
    "\n",
    "locations = [\n",
    "    '/Users/cody/github/etcbc/bhsa/tf/2021',\n",
    "    '/Users/cody/github/etcbc/genre_synvar/tf/2021',\n",
    "]\n",
    "\n",
    "edit_actions = [\n",
    "    editAction(\n",
    "        feature_updates={\n",
    "            'function': {974852: 'LocaTime'},\n",
    "            'prep_type': {974852: 'B_simul'},\n",
    "        },\n",
    "        edge_updates={\n",
    "            'oslots': {\n",
    "                484384: {283995, 283996, 283997, 283998, \n",
    "                         283999, 284000, 284001},\n",
    "                484385: {284002, 284003, 284004, 284005},\n",
    "            },\n",
    "        },\n",
    "    ),\n",
    "    editAction(\n",
    "        deletions={454316},\n",
    "        edge_updates={\n",
    "            'oslots': {\n",
    "                454315: {143086, 143087, 143088, 143089,\n",
    "                         143090, 143091, 143092, 143093,\n",
    "                         143094, 143095, 143096, 143097,\n",
    "                         143098, 143099}\n",
    "            },\n",
    "        },\n",
    "    ),\n",
    "]\n",
    "\n",
    "\n",
    "update_metadata = {\n",
    "    'omap@2021-KT': {\n",
    "        'description': 'Mapping between nodes in BHSA 2021 version to BHSA Kingham Thesis version',\n",
    "        'valueType': 'int',\n",
    "    },\n",
    "    'prep_type': {'description': 'test123', 'valueType': 'str'},\n",
    "}\n",
    "\n",
    "corpus_builder = ThesisCorpusBuilder(\n",
    "    locations,\n",
    "    book_limit=None,\n",
    "    update_metadata=update_metadata,\n",
    "    delete_features=delete_features,\n",
    "    rename_features=rename_features,\n",
    "    edit_actions=edit_actions,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "0071e245-b7eb-4059-b187-0e4d63de8201",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading TF corpus...\n",
      "  1.91s Feature overview: 110 for nodes; 6 for edges; 1 configs; 9 computed\n",
      "Getting a copy of the corpus...\n",
      "Filtering the nodes...\n",
      "Applying node edits...\n",
      "Reindexing the nodes...\n",
      "Rebuilding metadata...\n",
      "Refactoring features...\n",
      "Saving new corpus...\n",
      "  0.00s Not all of the warp features otype and oslots are present in\n",
      "test_corpus\n",
      "  0.00s Only the Feature and Edge APIs will be enabled\n",
      "  0.00s Warp feature \"otext\" not found. Working without Text-API\n",
      "\n",
      "  0.00s Exporting 79 node and 3 edge and 1 config features to test_corpus:\n",
      "  0.00s VALIDATING oslots feature\n",
      "  0.07s VALIDATING oslots feature\n",
      "  0.07s maxSlot=     426590\n",
      "  0.07s maxNode=    1441824\n",
      "  0.14s OK: oslots is valid\n",
      "   |     0.00s T book                 to test_corpus\n",
      "   |     0.01s T chapter              to test_corpus\n",
      "   |     0.04s T code                 to test_corpus\n",
      "   |     0.26s T det                  to test_corpus\n",
      "   |     0.04s T domain               to test_corpus\n",
      "   |     0.20s T freq_lex             to test_corpus\n",
      "   |     0.20s T freq_occ             to test_corpus\n",
      "   |     0.12s T function             to test_corpus\n",
      "   |     0.21s T g_cons               to test_corpus\n",
      "   |     0.24s T g_cons_utf8          to test_corpus\n",
      "   |     0.22s T g_lex                to test_corpus\n",
      "   |     0.24s T g_lex_utf8           to test_corpus\n",
      "   |     0.20s T g_nme                to test_corpus\n",
      "   |     0.21s T g_nme_utf8           to test_corpus\n",
      "   |     0.20s T g_pfm                to test_corpus\n",
      "   |     0.20s T g_pfm_utf8           to test_corpus\n",
      "   |     0.20s T g_prs                to test_corpus\n",
      "   |     0.20s T g_prs_utf8           to test_corpus\n",
      "   |     0.20s T g_uvf                to test_corpus\n",
      "   |     0.19s T g_uvf_utf8           to test_corpus\n",
      "   |     0.20s T g_vbe                to test_corpus\n",
      "   |     0.20s T g_vbe_utf8           to test_corpus\n",
      "   |     0.19s T g_vbs                to test_corpus\n",
      "   |     0.19s T g_vbs_utf8           to test_corpus\n",
      "   |     0.22s T g_word               to test_corpus\n",
      "   |     0.25s T g_word_utf8          to test_corpus\n",
      "   |     0.01s T genre                to test_corpus\n",
      "   |     0.22s T gloss                to test_corpus\n",
      "   |     0.21s T gn                   to test_corpus\n",
      "   |     0.04s T instruction          to test_corpus\n",
      "   |     0.05s T is_root              to test_corpus\n",
      "   |     0.04s T kind                 to test_corpus\n",
      "   |     0.19s T kq_hybrid            to test_corpus\n",
      "   |     0.19s T kq_hybrid_utf8       to test_corpus\n",
      "   |     0.03s T label                to test_corpus\n",
      "   |     0.22s T lex                  to test_corpus\n",
      "   |     0.21s T lex0                 to test_corpus\n",
      "   |     0.24s T lex_utf8             to test_corpus\n",
      "   |     0.20s T lexeme_count         to test_corpus\n",
      "   |     0.21s T ls                   to test_corpus\n",
      "   |     0.02s T nametype             to test_corpus\n",
      "   |     0.21s T nme                  to test_corpus\n",
      "   |     0.20s T nu                   to test_corpus\n",
      "   |     0.16s T otype                to test_corpus\n",
      "   |     0.04s T pargr                to test_corpus\n",
      "   |     0.21s T pdp                  to test_corpus\n",
      "   |     0.21s T pfm                  to test_corpus\n",
      "   |     0.00s T prep_type            to test_corpus\n",
      "   |     0.22s T prs                  to test_corpus\n",
      "   |     0.20s T prs_gn               to test_corpus\n",
      "   |     0.20s T prs_nu               to test_corpus\n",
      "   |     0.21s T prs_ps               to test_corpus\n",
      "   |     0.20s T ps                   to test_corpus\n",
      "   |     0.00s T qere                 to test_corpus\n",
      "   |     0.00s T qere_trailer         to test_corpus\n",
      "   |     0.00s T qere_trailer_utf8    to test_corpus\n",
      "   |     0.00s T qere_utf8            to test_corpus\n",
      "   |     0.20s T rank_lex             to test_corpus\n",
      "   |     0.20s T rank_occ             to test_corpus\n",
      "   |     0.35s T rela                 to test_corpus\n",
      "   |     0.04s T root                 to test_corpus\n",
      "   |     0.21s T sp                   to test_corpus\n",
      "   |     0.20s T st                   to test_corpus\n",
      "   |     0.21s T suffix_gender        to test_corpus\n",
      "   |     0.21s T suffix_number        to test_corpus\n",
      "   |     0.21s T suffix_person        to test_corpus\n",
      "   |     0.04s T tab                  to test_corpus\n",
      "   |     0.20s T trailer              to test_corpus\n",
      "   |     0.21s T trailer_utf8         to test_corpus\n",
      "   |     0.04s T txt                  to test_corpus\n",
      "   |     0.34s T typ                  to test_corpus\n",
      "   |     0.21s T uvf                  to test_corpus\n",
      "   |     0.21s T vbe                  to test_corpus\n",
      "   |     0.21s T vbs                  to test_corpus\n",
      "   |     0.01s T verse                to test_corpus\n",
      "   |     0.21s T voc_lex              to test_corpus\n",
      "   |     0.24s T voc_lex_utf8         to test_corpus\n",
      "   |     0.21s T vs                   to test_corpus\n",
      "   |     0.21s T vt                   to test_corpus\n",
      "   |     0.20s T mother               to test_corpus\n",
      "   |     0.98s T omap@2021-KT         to test_corpus\n",
      "   |     1.23s T oslots               to test_corpus\n",
      "   |     0.00s M otext                to test_corpus\n",
      "    16s Exported 79 node features and 3 edge features and 1 config features to test_corpus\n"
     ]
    }
   ],
   "source": [
    "test_corpus = corpus_builder.build('test_corpus')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e5e76a4-1fbe-41d6-a225-91083dcef0cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_builder.tf_fabric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "ecfc0fa7-1881-4671-b193-70d56d022f16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{454315: None}"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_corpus['edgeFeatures']['omap@2021-KT'].get(655162)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "3320f772-f810-4d17-9311-3e2e7b1215d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'בַּיֹּ֤ום הַהוּא֙ אָקִ֣ים אֶל־עֵלִ֔י אֵ֛ת כָּל־'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bhsa.T.text(454315)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3d34ba29-1702-4b32-84f3-418d0e4e3ba3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'בַּיֹּ֤ום הַהוּא֙ אָקִ֣ים אֶל־עֵלִ֔י אֵ֛ת כָּל־'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bhsa.T.text(454315)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e19d7f7e-753d-45c4-8206-5db5e2e761a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(143096, 143097, 143098, 143099)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bhsa.L.d(454316, 'word')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1f24f9dd-07fc-4fd1-aae5-4db7a2ff68ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(143086,\n",
       " 143087,\n",
       " 143088,\n",
       " 143089,\n",
       " 143090,\n",
       " 143091,\n",
       " 143092,\n",
       " 143093,\n",
       " 143094,\n",
       " 143095)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bhsa.L.d(454315, 'word')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "07c314a6-54b5-47c0-8099-76853ef6b186",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(543346,)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bhsa.Es('mother').f(543347)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1e575f2e-c810-4d2b-9f28-ab3c73a7e27d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'בַּיֹּ֤ום הַהוּא֙ אָקִ֣ים אֶל־עֵלִ֔י אֵ֛ת כָּל־'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bhsa.T.text(543346)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c65568cf-ee0e-4ad3-98f0-05dcd786ed36",
   "metadata": {},
   "outputs": [],
   "source": [
    "bhsa = corpus_builder.tf_api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2a236abf-a991-4268-ab3c-8c4492899323",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((454315, None),)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bhsa.Es('omap@c-2021').f(454299)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "82eae2be-160b-4583-b3c4-8c6555a916eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   |     1.29s T otype                from test_corpus\n",
      "   |       16s T oslots               from test_corpus\n",
      "   |     0.01s T qere_utf8            from test_corpus\n",
      "   |     1.47s T g_lex_utf8           from test_corpus\n",
      "   |     0.06s T verse                from test_corpus\n",
      "   |     1.54s T g_word               from test_corpus\n",
      "   |     0.01s T qere_trailer         from test_corpus\n",
      "   |     1.42s T g_cons               from test_corpus\n",
      "   |     1.38s T lex                  from test_corpus\n",
      "   |     0.00s T book                 from test_corpus\n",
      "   |     1.39s T lex_utf8             from test_corpus\n",
      "   |     1.42s T g_cons_utf8          from test_corpus\n",
      "   |     0.01s T qere_trailer_utf8    from test_corpus\n",
      "   |     1.41s T g_lex                from test_corpus\n",
      "   |     1.24s T trailer_utf8         from test_corpus\n",
      "   |     1.23s T trailer              from test_corpus\n",
      "   |     0.01s T qere                 from test_corpus\n",
      "   |     1.54s T g_word_utf8          from test_corpus\n",
      "   |     1.41s T voc_lex_utf8         from test_corpus\n",
      "   |     0.06s T chapter              from test_corpus\n",
      "   |      |     0.33s C __levels__           from otype, oslots, otext\n",
      "   |      |     8.40s C __order__            from otype, oslots, __levels__\n",
      "   |      |     0.31s C __rank__             from otype, __order__\n",
      "   |      |       20s C __levUp__            from otype, oslots, __rank__\n",
      "   |      |       16s C __levDown__          from otype, __levUp__, __rank__\n",
      "   |      |     0.71s C __characters__       from otext\n",
      "   |      |     4.36s C __boundary__         from otype, oslots, __rank__\n",
      "   |      |     0.08s C __sections__         from otype, oslots, otext, __levUp__, __levels__, book, chapter, verse\n",
      " 1m 23s Feature overview: 79 for nodes; 3 for edges; 1 configs; 9 computed\n",
      "   |     0.25s T code                 from test_corpus\n",
      "   |     1.63s T det                  from test_corpus\n",
      "   |     0.27s T domain               from test_corpus\n",
      "   |     1.20s T freq_lex             from test_corpus\n",
      "   |     1.18s T freq_occ             from test_corpus\n",
      "   |     0.80s T function             from test_corpus\n",
      "   |     1.20s T g_nme                from test_corpus\n",
      "   |     1.22s T g_nme_utf8           from test_corpus\n",
      "   |     1.14s T g_pfm                from test_corpus\n",
      "   |     1.13s T g_pfm_utf8           from test_corpus\n",
      "   |     1.14s T g_prs                from test_corpus\n",
      "   |     1.14s T g_prs_utf8           from test_corpus\n",
      "   |     1.11s T g_uvf                from test_corpus\n",
      "   |     1.11s T g_uvf_utf8           from test_corpus\n",
      "   |     1.15s T g_vbe                from test_corpus\n",
      "   |     1.12s T g_vbe_utf8           from test_corpus\n",
      "   |     1.11s T g_vbs                from test_corpus\n",
      "   |     1.12s T g_vbs_utf8           from test_corpus\n",
      "   |     0.07s T genre                from test_corpus\n",
      "   |     1.41s T gloss                from test_corpus\n",
      "   |     1.32s T gn                   from test_corpus\n",
      "   |     0.28s T instruction          from test_corpus\n",
      "   |     0.28s T is_root              from test_corpus\n",
      "   |     0.28s T kind                 from test_corpus\n",
      "   |     1.12s T kq_hybrid            from test_corpus\n",
      "   |     1.12s T kq_hybrid_utf8       from test_corpus\n",
      "   |     0.23s T label                from test_corpus\n",
      "   |     1.40s T lex0                 from test_corpus\n",
      "   |     1.19s T lexeme_count         from test_corpus\n",
      "   |     1.31s T ls                   from test_corpus\n",
      "   |     1.02s T mother               from test_corpus\n",
      "   |     0.13s T nametype             from test_corpus\n",
      "   |     1.28s T nme                  from test_corpus\n",
      "   |     1.32s T nu                   from test_corpus\n",
      "   |     8.49s T omap@2021-KT         from test_corpus\n",
      "   |     0.29s T pargr                from test_corpus\n",
      "   |     1.33s T pdp                  from test_corpus\n",
      "   |     1.31s T pfm                  from test_corpus\n",
      "   |     0.00s T prep_type            from test_corpus\n",
      "   |     1.33s T prs                  from test_corpus\n",
      "   |     1.31s T prs_gn               from test_corpus\n",
      "   |     1.31s T prs_nu               from test_corpus\n",
      "   |     1.31s T prs_ps               from test_corpus\n",
      "   |     1.30s T ps                   from test_corpus\n",
      "   |     1.15s T rank_lex             from test_corpus\n",
      "   |     1.15s T rank_occ             from test_corpus\n",
      "   |     2.23s T rela                 from test_corpus\n",
      "   |     0.26s T root                 from test_corpus\n",
      "   |     1.33s T sp                   from test_corpus\n",
      "   |     1.32s T st                   from test_corpus\n",
      "   |     1.31s T suffix_gender        from test_corpus\n",
      "   |     1.31s T suffix_number        from test_corpus\n",
      "   |     1.33s T suffix_person        from test_corpus\n",
      "   |     0.24s T tab                  from test_corpus\n",
      "   |     0.27s T txt                  from test_corpus\n",
      "   |     2.19s T typ                  from test_corpus\n",
      "   |     1.31s T uvf                  from test_corpus\n",
      "   |     1.31s T vbe                  from test_corpus\n",
      "   |     1.32s T vbs                  from test_corpus\n",
      "   |     1.42s T voc_lex              from test_corpus\n",
      "   |     1.32s T vs                   from test_corpus\n",
      "   |     1.34s T vt                   from test_corpus\n"
     ]
    }
   ],
   "source": [
    "# load the corpus and test\n",
    "test_fabric = Fabric('test_corpus')\n",
    "test_api = test_fabric.loadAll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "7966e1d0-7d7b-4117-b6c1-a038844d07bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(655163,)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_api.Es('omap@2021-KT').t(454317)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "d7e47cc0-6ca2-4953-a9be-469a3b56e2fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(519398,)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_api.L.u(655162, 'sentence')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "75b7496c-5ade-4656-ace0-5ba1709812b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'בַּיֹּ֤ום הַהוּא֙ אָקִ֣ים אֶל־עֵלִ֔י אֵ֛ת כָּל־אֲשֶׁ֥ר דִּבַּ֖רְתִּי אֶל־בֵּיתֹ֑ו '"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_api.T.text(655162)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "6aec2ffc-b37d-4a84-9269-e46cdbf9521e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'בַּיֹּ֤ום הַהוּא֙ אָקִ֣ים אֶל־עֵלִ֔י אֵ֛ת כָּל־אֲשֶׁ֥ר דִּבַּ֖רְתִּי אֶל־בֵּיתֹ֑ו הָחֵ֖ל וְכַלֵּֽה׃ '"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_api.T.text(519398)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "79ccb855-6096-429e-885b-a3c8deff896d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'הָחֵ֖ל '"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_api.T.text(655163)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2fbd1c5e-c11d-4f94-9771-ea255b2fc2f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "974850\n",
      "Conj\n",
      "וְ\n",
      "\n",
      "974851\n",
      "Pred\n",
      "הָיָ֣ה׀ \n",
      "\n",
      "974852\n",
      "Time\n",
      "בַּיֹּ֣ום הַה֗וּא \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for ph in test_api.L.d(685231, 'phrase'):\n",
    "    print(ph)\n",
    "    print(test_api.F.function.v(ph))\n",
    "    print(test_api.T.text(ph))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e411600d-7056-44a3-82ec-c844cbbb6f3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(284002, 284003, 284004, 284005)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_api.L.d(685232, 'word')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "63573ede-7cdf-49bd-a3bd-f806209489ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'מִשְׁפַּ֨חַת בֵּית־דָּוִ֤יד לְבָד֙ וּנְשֵׁיהֶ֣ם לְבָ֔ד מִשְׁפַּ֤חַת בֵּית־נָתָן֙ לְבָ֔ד וּנְשֵׁיהֶ֖ם לְבָֽד׃ '"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_api.T.text(484385)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "704d6dc0-ac42-4cdb-bb37-94cb09fa7086",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_api.T.text(484384)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cacb11e3-4b35-435e-993d-31a34826a8f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "bhsa = corpus_builder.tf_api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b4896c3-b7e6-4781-ab20-d95a113665a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'clause'"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bhsa.F.mother_object_type.v(484385)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f765b548-b54b-43c6-9a1a-304d5d6b85e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'xYqX'"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bhsa.F.typ.v(484385)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "285e3c95-e0fb-4b08-96d7-ec2eb7251683",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((484385, None),)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bhsa.Es('omap@c-2021').f(484368)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73f96e8e-e733-404d-9553-e2808562687b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'בַּיֹּ֣ום הַה֗וּא יַעֲל֤וּ דְבָרִים֙ עַל־לְבָבֶ֔ךָ '"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bhsa.T.text(484385)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d6bbc67-7481-494d-b347-d150222ee30c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1213703,)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bhsa.E.functional_parent.f(484386)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "fb94c652-1069-4efa-abc9-d37456c2d1dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'וְחָשַׁבְתָּ֖ מַחֲשֶׁ֥בֶת רָעָֽה׃ '"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bhsa.T.text(1213703)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "d3b6acbf-7ce4-4b9b-b9b2-9fa237b891f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sentence'"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bhsa.F.otype.v(1213703)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "9c562fd3-ccbb-41a7-b87d-b72f7f911524",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(574184, 819188, 819189, 819190)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bhsa.E.functional_parent.t(484386)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d9cd2f9e-37e8-4f5a-8bf7-4df009021080",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all clause annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "ca1f79d7-8ac3-4826-b04d-1dd6d79dfbe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_copy = get_copy_of_corpus(corpus_builder.tf_fabric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "c0d59c57-9b0f-4668-a42b-f5c895e14aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_all_features(node, corpus_copy):\n",
    "    all_feats = {'nodeFeatures': set(), 'edgeFeatures': set()}\n",
    "    for feat_type in ('nodeFeatures', 'edgeFeatures'):\n",
    "        for feature, node_dict in corpus_copy[feat_type].items():\n",
    "            if node in node_dict:\n",
    "                all_feats[feat_type].add(feature)\n",
    "    return all_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "aaec02c1-e252-44f3-bae1-bdcf62ef44fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'nodeFeatures': {'dist',\n",
       "  'dist_unit',\n",
       "  'domain',\n",
       "  'kind',\n",
       "  'mother_object_type',\n",
       "  'number',\n",
       "  'otype',\n",
       "  'rela',\n",
       "  'txt',\n",
       "  'typ'},\n",
       " 'edgeFeatures': {'functional_parent',\n",
       "  'omap@2017-2021',\n",
       "  'omap@c-2021',\n",
       "  'oslots'}}"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_all_features(484385, corpus_copy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "25df501a-8e49-4972-ba2b-ae62367a9f95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'nodeFeatures': {'det',\n",
       "  'dist',\n",
       "  'dist_unit',\n",
       "  'number',\n",
       "  'otype',\n",
       "  'rela',\n",
       "  'typ'},\n",
       " 'edgeFeatures': {'distributional_parent',\n",
       "  'functional_parent',\n",
       "  'omap@2017-2021',\n",
       "  'omap@c-2021',\n",
       "  'oslots'}}"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_all_features(1081944, corpus_copy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1c33243-7375-458d-b486-8a975fe060c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "bhsa.E."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "51e6c044-acf1-42f0-adfb-aa3eff0bac7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(484385,)"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bhsa.E.functional_parent.f(819184)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "54070052-93c4-45ae-aaa7-a5a8c5a0b561",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'בַּיֹּ֣ום הַה֗וּא '"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bhsa.T.text(819184)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "41e2b017-bab2-4d29-ba8c-7d57aa52224c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'phrase'"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bhsa.F.otype.v(484385)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "1c86debf-be45-4885-a757-74768c582ef3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'בַּיֹּ֣ום הַה֗וּא יַעֲל֤וּ דְבָרִים֙ עַל־לְבָבֶ֔ךָ '"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bhsa.T.text(484385)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "f72087e7-0214-4577-b630-abbf74d0ebab",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_delete = {\n",
    "    'dist', 'dist_unit', \n",
    "    'mother_object_type', 'number',\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "719a626a-328b-44f8-8f58-a7b7cfa63947",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'nodeFeatures': {'dist',\n",
       "  'dist_unit',\n",
       "  'domain',\n",
       "  'kind',\n",
       "  'mother_object_type',\n",
       "  'number',\n",
       "  'otype',\n",
       "  'rela',\n",
       "  'txt',\n",
       "  'typ'},\n",
       " 'edgeFeatures': {'functional_parent',\n",
       "  'omap@2017-2021',\n",
       "  'omap@c-2021',\n",
       "  'oslots'}}"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_cl_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "90517fbf-d31f-4df7-86a5-0315b1a60209",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "283997 בַּ\n",
      "283998 \n",
      "283999 יֹּ֣ום \n",
      "284000 הַ\n",
      "284001 ה֗וּא \n",
      "284002 יַעֲל֤וּ \n",
      "284003 דְבָרִים֙ \n",
      "284004 עַל־\n",
      "284005 לְבָבֶ֔ךָ \n"
     ]
    }
   ],
   "source": [
    "for slot in bhsa.L.d(484385, 'word'):\n",
    "    print(slot, bhsa.T.text(slot))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "008a4667-1d16-4a50-90c6-3954b70be53d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "283995 וְ\n",
      "283996 הָיָ֣ה׀ \n"
     ]
    }
   ],
   "source": [
    "for slot in bhsa.L.d(484384, 'word'):\n",
    "    print(slot, bhsa.T.text(slot))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "1128f254-fe3d-4465-8a8a-11b453fefa0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(819184,)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bhsa.L.u(283997, 'phrase')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "c17d9f52-295a-4cf9-9bd1-7ba16e98a50c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1081944,)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bhsa.L.u(283997, 'phrase_atom')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "11ed1995-ffb5-4b9b-8b00-24cb384a7fad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'וְהָיָ֣ה׀ '"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bhsa.T.text(484384)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "cffe89d3-a170-4962-be03-295f487859cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'בַּיֹּ֣ום הַה֗וּא יַעֲל֤וּ דְבָרִים֙ עַל־לְבָבֶ֔ךָ '"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bhsa.T.text(484385)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "64ab456f-206d-4d57-8fc1-6f88efcce1b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "ea273eef-b1ed-41e4-bad5-18c11ddc34a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.editAction at 0x3368a95a0>"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b42bf6-7ed2-4c98-99ed-6f126500b2b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "fc554543-4690-4ae9-9fc1-4fe3c1269b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "shift_slots = (\n",
    "    (\n",
    "        # shift from left to right\n",
    "        # left, slots, right\n",
    "        [\n",
    "            484385, \n",
    "            {283997, 283998, 283999, 284000, 284001}, \n",
    "            484384\n",
    "        ],\n",
    "        \n",
    "        # node feature updates\n",
    "        {},\n",
    "        \n",
    "        # edge feature updates\n",
    "        {},\n",
    "    ),\n",
    ")\n",
    "\n",
    "merges = (\n",
    "    (\n",
    "        # nodes to merge, to leftmost\n",
    "        [],\n",
    "        # node feature updates\n",
    "        {},\n",
    "        # edge feature updates\n",
    "        {},\n",
    "    ),\n",
    ")\n",
    "\n",
    "splits = [\n",
    "    # split operation\n",
    "    # actions:\n",
    "    # 1) add new nodes with new oslots\n",
    "    # 2) update oslots for node\n",
    "    # 3) update features for all new nodes and for first\n",
    "    [\n",
    "        # node\n",
    "        427559,\n",
    "        # new oslot map\n",
    "        [(1, 2, 3, 4), (5, 6, 7)],\n",
    "        # features for new nodes\n",
    "        [],\n",
    "        # edges for new nodes\n",
    "        [],\n",
    "        # (existing) node feature updates\n",
    "        [],\n",
    "        # edge feature updates\n",
    "        [],\n",
    "    ],\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2f9bd091-0733-401c-939c-85cfd27f087f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "typing.Tuple[typing.Tuple[int], typing.Dict[str, typing.Union[str, int]], typing.Union[typing.Set[int], typing.Dict[int, typing.Union[str, int]]]]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d95457a6-c240-406b-89c6-cdf0ba3980d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "typing.Tuple[int, typing.List[typing.Dict[str, typing.Union[str, int]]], typing.List[typing.Union[typing.Set[int], typing.Dict[int, typing.Union[str, int]]]]]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c895d373-21c3-490c-bba5-e1d9bcc77b94",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "thesis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
