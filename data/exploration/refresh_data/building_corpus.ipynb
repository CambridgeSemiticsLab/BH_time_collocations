{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "14bf8e61-97b1-4303-a872-e98e9c84a058",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import collections\n",
    "\n",
    "from copy import deepcopy\n",
    "from tf.fabric import Fabric\n",
    "from tf.core.api import Api\n",
    "from tf.convert.walker import CV\n",
    "from pathlib import Path\n",
    "from functools import cmp_to_key\n",
    "from typing import Dict, Tuple, Union, Optional, Set, List"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b960954-524e-47c3-8fa5-c38f2840b2b0",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "67ab5b74-90b3-423b-a351-a887198ce5c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "BHSA_CORE_DATA = Path('/Users/cody/github/etcbc/bhsa')\n",
    "BHSA_TF = BHSA_CORE_DATA / 'tf/2021'\n",
    "BHSA_YAML = BHSA_CORE_DATA / 'yaml'\n",
    "BHSA_METADATA_FILES = ['core.yaml', 'lexicon.yaml', 'ketivqere.yaml', 'paragraph.yaml', 'stats.yaml']\n",
    "BHSA_METADATA_PATHS = [BHSA_YAML / file for file in BHSA_METADATA_FILES]\n",
    "\n",
    "BHSA_GENERIC = BHSA_YAML / 'generic.yaml'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c47272b0-aac9-4d89-b39f-be4ff51c16d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_yaml(filepath):\n",
    "    \"\"\"Load yaml config as dict.\"\"\"\n",
    "    with open(filepath, 'r') as infile:\n",
    "        return yaml.load(infile, Loader=yaml.FullLoader)\n",
    "    \n",
    "\n",
    "def load_all_feature_metadata(feature_metadata_paths):\n",
    "    \"\"\"Load all feature metadata into a single dictionary.\"\"\"\n",
    "    return {\n",
    "        feature: value\n",
    "        for path in feature_metadata_paths\n",
    "        for feature, value in load_yaml(path).items()\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0fbc2751-415a-47da-8d7e-bebfbcc406e1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# tf_bhsa = Fabric('/Users/cody/github/etcbc/bhsa/tf/2021')\n",
    "# bhsa = tf_bhsa.loadAll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f334c855-0ce7-4b30-bcbd-fd73359c2886",
   "metadata": {},
   "outputs": [],
   "source": [
    "BHSAKT_METADATA = {\n",
    "    \"corpus\": \"BHSA-Kingham-thesis\",\n",
    "    \"description\": \"A modified version of the ETCBC's BHSA for my Cambridge PhD thesis\",\n",
    "    \"version\": \"1.0\",\n",
    "    \"editor\": \"Cody Kingham\",\n",
    "    \"source\": \"Eep Talstra Centre for Bible and Computer\",\n",
    "    \"source-url\": \"https://github.com/etcbc/bhsa\",\n",
    "    \"encoders\": \"Constantijn Sikkel (QDF), Ulrik Petersen (MQL) and Dirk Roorda (TF)\",\n",
    "}\n",
    "\n",
    "\n",
    "GENERIC_META = load_yaml(BHSA_GENERIC)\n",
    "GENERIC_META.update({\n",
    "    'dateWritten': None,\n",
    "    'writtenBy': None,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "884aea51-b502-41ee-b1c0-2cf65455a0f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dataset': 'BHSA',\n",
       " 'datasetName': 'Biblia Hebraica Stuttgartensia Amstelodamensis',\n",
       " 'author': 'Eep Talstra Centre for Bible and Computer',\n",
       " 'encoders': 'Constantijn Sikkel (QDF), Ulrik Petersen (MQL) and Dirk Roorda (TF)',\n",
       " 'website': 'https://shebanq.ancient-data.org',\n",
       " 'email': 'shebanq@ancient-data.org',\n",
       " 'dateWritten': None,\n",
       " 'writtenBy': None}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GENERIC_META"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7ddc0f73-a54b-439a-883c-a81f3b4d5130",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_chunk(node):\n",
    "    slots = bhsa.L.d(node, 'word')\n",
    "    return (node, set(slots))\n",
    "\n",
    "\n",
    "def _canonical_order(node_chunk_a, node_chunk_b):\n",
    "    \"\"\"Sort items in canonical sorting order.\"\"\"\n",
    "    na, prec_a, slotsA = node_chunk_a\n",
    "    nb, prec_b, slotsB = node_chunk_b\n",
    "    \n",
    "    # compare based on node precedence\n",
    "    if prec_a > prec_b:\n",
    "        return -1\n",
    "    elif prec_b > prec_a:\n",
    "        return 1\n",
    "    \n",
    "    # compare based on slots\n",
    "    else:\n",
    "        # slots are equivalent\n",
    "        if slotsA == slotsB:\n",
    "            return 0\n",
    "\n",
    "        # a is subset of b\n",
    "        aWithoutB = slotsA - slotsB\n",
    "        if not aWithoutB:\n",
    "            return 1\n",
    "\n",
    "        # b is subset of a\n",
    "        bWithoutA = slotsB - slotsA\n",
    "        if not bWithoutA:\n",
    "            return -1\n",
    "\n",
    "        # compare based on slots\n",
    "        aMin = min(aWithoutB)\n",
    "        bMin = min(bWithoutA)\n",
    "        return -1 if aMin < bMin else 1\n",
    "\n",
    "\n",
    "canonical_order = cmp_to_key(_canonical_order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6f28db59-754f-4f56-b9f9-634a588a2ee2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(2, 10, {1, 2, 3}),\n",
       " (1, 9, {1, 2, 3}),\n",
       " (3, 6, {1, 2, 3}),\n",
       " (3, 6, {1, 2}),\n",
       " (3, 5, {1, 2}),\n",
       " (3, 5, {3, 4})]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test the canonical sorting\n",
    "\n",
    "test = [\n",
    "    (1, 9, {1, 2, 3}),\n",
    "    (2, 10, {1, 2, 3}),\n",
    "    (3, 5, {3, 4}),\n",
    "    (3, 5, {1, 2}),\n",
    "    (3, 6, {1, 2, 3}),\n",
    "    (3, 6, {1, 2})\n",
    "]\n",
    "\n",
    "sorted(test, key=canonical_order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d655417b-80a3-4ce4-bd56-b0af3a1458f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _copy_meta_dicts(feature_dict):\n",
    "    \"\"\"Extract metakwargs.\"\"\"\n",
    "    return {\n",
    "        feat: deepcopy(feat_obj.meta)\n",
    "        for feat, feat_obj in feature_dict.items()\n",
    "    }\n",
    "\n",
    "\n",
    "def _copy_feature_dicts(feature_dict):\n",
    "    \"\"\"Extract feature dicts.\"\"\"\n",
    "    return {\n",
    "        feat: dict(feat_obj.items())\n",
    "        for feat, feat_obj in feature_dict.items()\n",
    "    }\n",
    "\n",
    "    \n",
    "def get_copy_of_corpus(tf_fabric: Fabric):\n",
    "    \"\"\"Get a copy of a corpus's resources.\"\"\"\n",
    "    tf_api = tf_fabric.api\n",
    "    node_features = _copy_feature_dicts(tf_api.F.__dict__)\n",
    "    edge_features = _copy_feature_dicts(tf_api.E.__dict__)\n",
    "    metadata = {\n",
    "        **_copy_meta_dicts(tf_api.F.__dict__),\n",
    "        **_copy_meta_dicts(tf_api.E.__dict__),\n",
    "    }\n",
    "    metadata['otext'] = tf_fabric.features['otext'].metaData\n",
    "    return {\n",
    "        'nodeFeatures': node_features,\n",
    "        'edgeFeatures': edge_features,\n",
    "        'metaData': metadata,\n",
    "    }\n",
    "\n",
    "\n",
    "class ThesisCorpusBuilder:\n",
    "    \"\"\"Class for building thesis corpus.\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self, \n",
    "        locations: Optional[List[str]] = None,\n",
    "        book_limit: Optional[int] = None,\n",
    "        delete_features: Optional[Set[str]] = None,\n",
    "        rename_features: Optional[Dict[str, str]] = None,\n",
    "        tf_fabric: Optional[Fabric] = None,\n",
    "    ):\n",
    "        \"\"\"Initialize the thesis corpus builder.\"\"\"\n",
    "        self.locations = locations or ''\n",
    "        self.book_limit = book_limit\n",
    "        self.delete_features = delete_features or set()\n",
    "        self.rename_features = rename_features or {}\n",
    "        self.tf_fabric = tf_fabric\n",
    "        self.tf_api = tf_fabric.api if tf_fabric else None\n",
    "\n",
    "    def _get_keep_node_set(self):\n",
    "        \"\"\"Get set of nodes to keep.\"\"\"\n",
    "        keep_nodes = set()\n",
    "        max_slot = 0\n",
    "        book_limit = (\n",
    "            self.tf_api.T.nodeFromSection((self.book_limit,))\n",
    "            if self.book_limit else None\n",
    "        )\n",
    "        for book_node in self.tf_api.F.otype.s('book'):\n",
    "            if book_limit and book_node > book_limit:\n",
    "                break\n",
    "            keep_nodes.add(book_node)\n",
    "            for node in self.tf_api.L.d(book_node):\n",
    "                keep_nodes.add(node)\n",
    "                if self.tf_api.F.otype.v(node) == 'word':\n",
    "                    max_slot = node\n",
    "        return keep_nodes, max_slot\n",
    "    \n",
    "    @staticmethod\n",
    "    def _filter_nodes_from_feature_dict(feature_dict, keep_nodes):\n",
    "        \"\"\"Filter keep nodes.\"\"\"\n",
    "        filtered_feature_dict = {}\n",
    "        for feature, node_dict in feature_dict.items():\n",
    "            filtered_feature_dict[feature] = {\n",
    "                node: feature\n",
    "                for node, feature in node_dict.items()\n",
    "                if node in keep_nodes\n",
    "            }\n",
    "        return filtered_feature_dict\n",
    "    \n",
    "    def _filter_feature_dict_nodes(self, corpus_data, keep_node_set):\n",
    "        \"\"\"Filter feature dict nodes.\"\"\"\n",
    "        corpus_data['nodeFeatures'] = self._filter_nodes_from_feature_dict(\n",
    "            corpus_data['nodeFeatures'],\n",
    "            keep_node_set,\n",
    "        )\n",
    "        corpus_data['edgeFeatures'] = self._filter_nodes_from_feature_dict(\n",
    "            corpus_data['edgeFeatures'],\n",
    "            keep_node_set\n",
    "        )\n",
    "    \n",
    "    def _rebuild_nodes_from_oslots(\n",
    "            self, \n",
    "            oslot_map,\n",
    "            otype_map,\n",
    "            max_slot,\n",
    "    ) -> Dict[int, int]:\n",
    "        \"\"\"Rebuild node numbering scheme from oslots.\"\"\"\n",
    "        # get sorted list of oslot data\n",
    "        oslots = []\n",
    "        for node, oslot_set in oslot_map.items():\n",
    "            otype = otype_map[node]\n",
    "            otype_rank = self.tf_api.Nodes.otypeRank[otype]\n",
    "            oslots.append((node, otype_rank, set(oslot_set)))\n",
    "        oslots.sort(key=canonical_order)\n",
    "        \n",
    "        # create mapping to new node numbers\n",
    "        new_node_map = {\n",
    "            old_node: (i + max_slot)\n",
    "            for i, (old_node, _, _) in enumerate(oslots, 1)\n",
    "        }\n",
    "        return new_node_map\n",
    "        \n",
    "    @staticmethod\n",
    "    def _reindex_node_features(old_node_features, remapper):\n",
    "        \"\"\"Reindex node features.\"\"\"\n",
    "        node_features = collections.defaultdict(dict)\n",
    "        for feature, node_dict in old_node_features.items():\n",
    "            for node, fvalue in node_dict.items():\n",
    "                node_features[feature][remapper(node)] = fvalue\n",
    "        return node_features\n",
    "    \n",
    "    @staticmethod\n",
    "    def _reindex_edge_features(old_edge_features, remapper):\n",
    "        \"\"\"Reindex edge features.\"\"\"\n",
    "        edge_features = collections.defaultdict(dict)\n",
    "        for feature, edge_dict in old_edge_features.items():\n",
    "            for node, edges in edge_dict.items():\n",
    "                if isinstance(edges, dict):\n",
    "                    edge_features[feature][remapper(node)] = {\n",
    "                        remapper(n) for n, v\n",
    "                        in edges.items()\n",
    "                    }\n",
    "                else:\n",
    "                    edge_features[feature][remapper(node)] = set(\n",
    "                        remapper(n) for n in edges\n",
    "                    )\n",
    "        return edge_features\n",
    "        \n",
    "    def _reindex_nodes(self, corpus_data, max_slot):\n",
    "        \"\"\"Reindex nodes.\"\"\"\n",
    "        # rebuild node numbering from oslot data\n",
    "        new_node_map = self._rebuild_nodes_from_oslots(\n",
    "            corpus_data['edgeFeatures']['oslots'],\n",
    "            corpus_data['nodeFeatures']['otype'],\n",
    "            max_slot,\n",
    "        )\n",
    "        \n",
    "        # remap all node features using the new numbering scheme\n",
    "        remapper = lambda node: new_node_map.get(node, node)\n",
    "        node_features = self._reindex_node_features(corpus_data['nodeFeatures'], remapper)\n",
    "        edge_features = self._reindex_edge_features(corpus_data['edgeFeatures'], remapper)\n",
    "\n",
    "        # apply the changes to the dict\n",
    "        corpus_data['nodeFeatures'] = node_features\n",
    "        corpus_data['edgeFeatures'] = edge_features\n",
    "\n",
    "    def _rebuild_metadata(self, corpus_data):\n",
    "        \"\"\"Remap metadata for this project.\"\"\"\n",
    "        new_metadata = {}\n",
    "        for feature, meta in corpus_data['metaData'].items():\n",
    "            unique_meta = {\n",
    "                k: v for k, v in meta.items()\n",
    "                if k not in GENERIC_META\n",
    "            }\n",
    "            new_metadata[feature] = {\n",
    "                **BHSAKT_METADATA,\n",
    "                **unique_meta,\n",
    "            }\n",
    "        corpus_data['metaData'] = new_metadata\n",
    "    \n",
    "    def _delete_features(self, corpus_data):\n",
    "        \"\"\"Remove features from the dataset.\"\"\"\n",
    "        for feature in self.delete_features:\n",
    "            for data_type, data_dict in corpus_data.items():                \n",
    "                if feature in data_dict:\n",
    "                    del data_dict[feature]\n",
    "\n",
    "    def _rename_features(self, corpus_data):\n",
    "        \"\"\"Rename features in the dataset.\"\"\"\n",
    "        for old_name, new_name in self.rename_features.items():\n",
    "            for data_type, data_dict in corpus_data.items():\n",
    "                if old_name in data_dict:    \n",
    "                    data_dict[new_name] = data_dict[old_name]\n",
    "                    del data_dict[old_name]\n",
    "\n",
    "    @staticmethod\n",
    "    def _clear_directory(dest_dir: str):\n",
    "        \"\"\"Empty a destination directory of old data.\"\"\"\n",
    "        for file in Path(dest_dir).glob('*.tf'):\n",
    "            file.unlink()\n",
    "            \n",
    "    def _load_tf_corpus(self):\n",
    "        \"\"\"Load Text Fabric corpus.\"\"\"\n",
    "        if not self.tf_fabric:\n",
    "            self.tf_fabric = Fabric(self.locations)\n",
    "            self.tf_api = self.tf_fabric.loadAll()\n",
    "                    \n",
    "    def build(self, dest_dir: str):\n",
    "        \"\"\"Build the corpus.\"\"\"\n",
    "        print('Loading TF corpus...')\n",
    "        self._load_tf_corpus()\n",
    "        \n",
    "        print('Getting a copy of the corpus...')\n",
    "        corpus_data = get_copy_of_corpus(self.tf_fabric)\n",
    "        \n",
    "        print('Filtering the nodes...')\n",
    "        keep_node_set, max_slot = self._get_keep_node_set()\n",
    "        self._filter_feature_dict_nodes(corpus_data, keep_node_set)\n",
    "        \n",
    "        print('Reindexing the nodes...')\n",
    "        self._reindex_nodes(corpus_data, max_slot)\n",
    "        \n",
    "        print('Rebuilding metadata...')\n",
    "        self._rebuild_metadata(corpus_data)\n",
    "        \n",
    "        print('Refactoring features...')\n",
    "        self._delete_features(corpus_data)\n",
    "        self._rename_features(corpus_data)\n",
    "        \n",
    "        print('Saving new corpus...')\n",
    "        self._clear_directory(dest_dir)\n",
    "        saver = Fabric(dest_dir)\n",
    "        saver.save(**corpus_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0cd001d0-8951-4b50-a102-54ec41e39a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "book_limit = '2_Kings'\n",
    "\n",
    "delete_features = {\n",
    "    'book@am', 'book@ar', 'book@bn', 'book@da',\n",
    "    'book@de', 'book@el', 'book@es', 'book@fa', \n",
    "    'book@fr', 'book@he', 'book@hi', 'book@id', \n",
    "    'book@ja', 'book@ko', 'book@la', 'book@nl', \n",
    "    'book@pa', 'book@pt', 'book@ru', 'book@sw', \n",
    "    'book@syc', 'book@tr', 'book@ur', 'book@yo', \n",
    "    'book@zh', 'book',\n",
    "}\n",
    "\n",
    "rename_features = {\n",
    "    'book@en': 'book',\n",
    "}\n",
    "\n",
    "locations = [\n",
    "    '/Users/cody/github/etcbc/bhsa/tf/2021',\n",
    "    '/Users/cody/github/etcbc/genre_synvar/tf/2021',\n",
    "]\n",
    "\n",
    "corpus_builder = ThesisCorpusBuilder(\n",
    "    locations,\n",
    "    book_limit=book_limit,\n",
    "    delete_features=delete_features,\n",
    "    rename_features=rename_features,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0071e245-b7eb-4059-b187-0e4d63de8201",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading TF corpus...\n",
      "  2.01s Feature overview: 110 for nodes; 6 for edges; 1 configs; 9 computed\n",
      "Getting a copy of the corpus...\n",
      "Filtering the nodes...\n",
      "Reindexing the nodes...\n",
      "Rebuilding metadata...\n",
      "Refactoring features...\n",
      "Saving new corpus...\n",
      "  0.00s Not all of the warp features otype and oslots are present in\n",
      "test_corpus\n",
      "  0.00s Only the Feature and Edge APIs will be enabled\n",
      "  0.00s Warp feature \"otext\" not found. Working without Text-API\n",
      "\n",
      "  0.00s Exporting 84 node and 6 edge and 1 config features to test_corpus:\n",
      "  0.00s VALIDATING oslots feature\n",
      "  0.03s VALIDATING oslots feature\n",
      "  0.03s maxSlot=     212072\n",
      "  0.03s maxNode=     694564\n",
      "  0.06s OK: oslots is valid\n",
      "   |     0.00s T book                 to test_corpus\n",
      "   |     0.01s T chapter              to test_corpus\n",
      "   |     0.02s T code                 to test_corpus\n",
      "   |     0.12s T det                  to test_corpus\n",
      "   |     0.17s T dist                 to test_corpus\n",
      "   |     0.15s T dist_unit            to test_corpus\n",
      "   |     0.02s T domain               to test_corpus\n",
      "   |     0.10s T freq_lex             to test_corpus\n",
      "   |     0.10s T freq_occ             to test_corpus\n",
      "   |     0.06s T function             to test_corpus\n",
      "   |     0.11s T g_cons               to test_corpus\n",
      "   |     0.12s T g_cons_utf8          to test_corpus\n",
      "   |     0.11s T g_lex                to test_corpus\n",
      "   |     0.12s T g_lex_utf8           to test_corpus\n",
      "   |     0.10s T g_nme                to test_corpus\n",
      "   |     0.11s T g_nme_utf8           to test_corpus\n",
      "   |     0.10s T g_pfm                to test_corpus\n",
      "   |     0.10s T g_pfm_utf8           to test_corpus\n",
      "   |     0.10s T g_prs                to test_corpus\n",
      "   |     0.10s T g_prs_utf8           to test_corpus\n",
      "   |     0.10s T g_uvf                to test_corpus\n",
      "   |     0.09s T g_uvf_utf8           to test_corpus\n",
      "   |     0.10s T g_vbe                to test_corpus\n",
      "   |     0.10s T g_vbe_utf8           to test_corpus\n",
      "   |     0.09s T g_vbs                to test_corpus\n",
      "   |     0.09s T g_vbs_utf8           to test_corpus\n",
      "   |     0.11s T g_word               to test_corpus\n",
      "   |     0.12s T g_word_utf8          to test_corpus\n",
      "   |     0.01s T genre                to test_corpus\n",
      "   |     0.11s T gloss                to test_corpus\n",
      "   |     0.10s T gn                   to test_corpus\n",
      "   |     0.02s T instruction          to test_corpus\n",
      "   |     0.02s T is_root              to test_corpus\n",
      "   |     0.02s T kind                 to test_corpus\n",
      "   |     0.09s T kq_hybrid            to test_corpus\n",
      "   |     0.09s T kq_hybrid_utf8       to test_corpus\n",
      "   |     0.01s T label                to test_corpus\n",
      "   |     0.10s T language             to test_corpus\n",
      "   |     0.10s T languageISO          to test_corpus\n",
      "   |     0.11s T lex                  to test_corpus\n",
      "   |     0.11s T lex0                 to test_corpus\n",
      "   |     0.12s T lex_utf8             to test_corpus\n",
      "   |     0.10s T lexeme_count         to test_corpus\n",
      "   |     0.10s T ls                   to test_corpus\n",
      "   |     0.05s T mother_object_type   to test_corpus\n",
      "   |     0.01s T nametype             to test_corpus\n",
      "   |     0.10s T nme                  to test_corpus\n",
      "   |     0.10s T nu                   to test_corpus\n",
      "   |     0.27s T number               to test_corpus\n",
      "   |     0.08s T otype                to test_corpus\n",
      "   |     0.02s T pargr                to test_corpus\n",
      "   |     0.10s T pdp                  to test_corpus\n",
      "   |     0.10s T pfm                  to test_corpus\n",
      "   |     0.10s T prs                  to test_corpus\n",
      "   |     0.10s T prs_gn               to test_corpus\n",
      "   |     0.10s T prs_nu               to test_corpus\n",
      "   |     0.10s T prs_ps               to test_corpus\n",
      "   |     0.10s T ps                   to test_corpus\n",
      "   |     0.00s T qere                 to test_corpus\n",
      "   |     0.00s T qere_trailer         to test_corpus\n",
      "   |     0.00s T qere_trailer_utf8    to test_corpus\n",
      "   |     0.00s T qere_utf8            to test_corpus\n",
      "   |     0.10s T rank_lex             to test_corpus\n",
      "   |     0.10s T rank_occ             to test_corpus\n",
      "   |     0.17s T rela                 to test_corpus\n",
      "   |     0.02s T root                 to test_corpus\n",
      "   |     0.10s T sp                   to test_corpus\n",
      "   |     0.10s T st                   to test_corpus\n",
      "   |     0.10s T suffix_gender        to test_corpus\n",
      "   |     0.10s T suffix_number        to test_corpus\n",
      "   |     0.10s T suffix_person        to test_corpus\n",
      "   |     0.02s T tab                  to test_corpus\n",
      "   |     0.10s T trailer              to test_corpus\n",
      "   |     0.11s T trailer_utf8         to test_corpus\n",
      "   |     0.02s T txt                  to test_corpus\n",
      "   |     0.16s T typ                  to test_corpus\n",
      "   |     0.10s T uvf                  to test_corpus\n",
      "   |     0.10s T vbe                  to test_corpus\n",
      "   |     0.10s T vbs                  to test_corpus\n",
      "   |     0.00s T verse                to test_corpus\n",
      "   |     0.11s T voc_lex              to test_corpus\n",
      "   |     0.12s T voc_lex_utf8         to test_corpus\n",
      "   |     0.10s T vs                   to test_corpus\n",
      "   |     0.10s T vt                   to test_corpus\n",
      "   |     0.38s T distributional_parent to test_corpus\n",
      "   |     0.56s T functional_parent    to test_corpus\n",
      "   |     0.09s T mother               to test_corpus\n",
      "   |     0.69s T omap@2017-2021       to test_corpus\n",
      "   |     0.69s T omap@c-2021          to test_corpus\n",
      "   |     0.57s T oslots               to test_corpus\n",
      "   |     0.00s M otext                to test_corpus\n",
      "    10s Exported 84 node features and 6 edge features and 1 config features to test_corpus\n"
     ]
    }
   ],
   "source": [
    "corpus_builder.build('test_corpus')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d3a9dc29-b338-44d5-ae40-86bb8fa8a020",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_fabric = corpus_builder.tf_fabric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "97578f1e-493a-4795-81de-a67249ea6699",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'prep'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_fabric.api.F.pdp.v(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "fc554543-4690-4ae9-9fc1-4fe3c1269b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "merges = [\n",
    "    # merge operation\n",
    "    # actions to take:\n",
    "    # 1) delete last nodes, leave first\n",
    "    # 2) update oslots for first node\n",
    "    # 3) delete all features for last nodes\n",
    "    # 4) update features for first\n",
    "    [\n",
    "        # nodes to merge\n",
    "        [427559, 427560],\n",
    "        # new features\n",
    "        ['XQtl', ...],\n",
    "        # new edges\n",
    "        [...],\n",
    "    ],\n",
    "]\n",
    "\n",
    "splits = [\n",
    "    # split operation\n",
    "    # actions:\n",
    "    # 1) add new nodes with new oslots\n",
    "    # 2) update oslots for node\n",
    "    # 3) update features for all new nodes and for first\n",
    "    [\n",
    "        # node\n",
    "        427559,\n",
    "        # new oslot map\n",
    "        [(1, 2, 3, 4), (5, 6, 7)],\n",
    "        # new cl features\n",
    "        [\n",
    "            ('XQtl', ...),\n",
    "        ],\n",
    "        # new cl edges\n",
    "        [\n",
    "            (...),\n",
    "            ...\n",
    "        ],\n",
    "    ],\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f9bd091-0733-401c-939c-85cfd27f087f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "thesis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
