"""
Snakefile for data analysis pipeline.
"""

from pathlib import Path

# locations for Text-Fabric BHSA data
bhsadata = [
    "/Users/cody/github/etcbc/bhsa/tf/c",
    "/Users/cody/github/etcbc/genre_synvar/tf/c",
]

home = Path.home()
github = home.joinpath('github')

# style sheets for styling dataset reports
# many of these point to Text-Fabric styles in order
# to style the HTML objects output by TF
stylesheets = [

    ("/Users/cody/github/CambridgeSemiticsLab"
     "/time_collocations/workflow/css/style.css"),

    ("/Users/cody/github/annotation"
     "/text-fabric/tf/server/static/base.css"),

    ("/Users/cody/github/annotation"
     "/text-fabric/tf/server/static/display.css"),

    ("/Users/cody/github/annotation"
     "/text-fabric/tf/server/static/highlight.css")

]

rule patch_function:
    input:
        bhsadata=bhsadata,
        editfuncts="data/edit_function.json",
    output:
        patched="../results/parsing/patched_function.json"
    script:
        "scripts/patch_function.py"

rule parse_pos:
    input:
        bhsadata=bhsadata,
        functions=rules.patch_function.output.patched,
        script1="scripts/parse_pos.py",
        script2="scripts/parsing/pos_parsing.py"
    output:
        slot2pos="../results/parsing/slot2pos.json",
        uniquepos="../results/parsing/uniquepos.txt"
    script:
        "scripts/parse_pos.py"

rule build_samples:
    input:
        bhsadata=bhsadata,
        script="scripts/sample_phrases.py",
        script2="scripts/sampling/phrase_sampling.py"
    output:
        samples="../results/samples/phrases.json",
        nosamples="../results/samples/reject_phrases.json"
    script:
        "scripts/sample_phrases.py"

rule sample_metrics:
    input:
        samples=rules.build_samples.output.samples,
        nosamples=rules.build_samples.output.nosamples,
        bhsadata=bhsadata
    params:
        outdir="../results/data_metrics"
    output:
        metrics="../results/data_metrics/phrase_samples.html"
    script:
        "scripts/measure_sample.py"

# directory to store plots produced in the data-metrics scripts
metric_plotsdir = (
    "/Users/cody/github/CambridgeSemiticsLab/time_collocations"
    "/results/data_metrics/plots"
)

# preprocess disambiguation data for phrase parsing
rule get_disambigs:
    input:
        bhsadata=bhsadata,
    output:
        accentseps="../results/disambigs/accentseps.json",
        paraseps="../results/disambigs/paraseps.json",
    script:
        "scripts/disambig.py"

rule parse_phrases:
    input:
        samples=rules.build_samples.output.samples,
        bhsadata=bhsadata,
        slot2pos=rules.parse_pos.output.slot2pos,
        functions=rules.patch_function.output.patched,
        paraseps=rules.get_disambigs.output.paraseps,
        editedges="data/edit_edges.json",
        styles=stylesheets,
        script="scripts/parsing/phatom_parser.py",
        script2="scripts/parsing/phatom_composer.py"
    params:
        plotsdir=metric_plotsdir 
    output:
        parsed_atoms="../results/parsing/phatom_parsings.json",
        unparsed_atoms="../results/parsing/phatom_notparsings.json",
        parsed_phrases="../results/parsing/phrase_parsings.json",
        parsemetrics="../results/data_metrics/unparsed_phrases.html",
        notparsemetrics="../results/data_metrics/parsed_phrases.html"
    script:
        "scripts/parse_phrases.py"

rule parse_times:
    input: 
        ph_parses=rules.parse_phrases.output.parsed_phrases,
        bhsadata=bhsadata,
        slot2pos=rules.parse_pos.output.slot2pos,
        functions=rules.patch_function.output.patched,
        lexmap="data/lexmap.json",
        styles=stylesheets,
        script="scripts/parsing/time_parser.py",
    params:
        plotsdir=metric_plotsdir
    output:
        parsed="../results/parsing/time_parsings.json",
        notparsed="../results/parsing/time_notparsings.json",
        parsemets="../results/data_metrics/time_parsings.html",
        catmets="../results/data_metrics/time_categories.html",
    script:
        "scripts/parse_times.py"

# human review process
rule eval_times:
    input:
        bhsadata=bhsadata,
        corrections="../results/parsing/time_corrections.json",
        parsed=rules.parse_times.output.parsed,
        translations="_private_/verse2text.json",
    params:
        ntoget=200,
    output:
        todo="notebooks/time_evaluation/to_eval.ipynb",
        evaled="../results/parsing/time_evaled.json",
    script:
        "scripts/eval_times.py"

rule build_dataset:
    input:
        bhsadata=bhsadata,
        timedata=rules.eval_times.output.evaled,
        phrasedata=rules.parse_phrases.output.parsed_phrases,
    output: 
        dataset="../results/analysis/time_dataset.csv"
    script:
        "scripts/build_dataset.py"
